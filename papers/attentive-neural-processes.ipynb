{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### Paper Information\n", "\n", "* Title: [Attentive Neural Processes](https://arxiv.org/abs/1901.05761) (ICLR 2019)\n", "* Authors: Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, Yee Whye Teh (2019)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Main Ideas\n", "\n", "* Neural Processes ([Garnelo et al., 2018](https://arxiv.org/abs/1807.01622)) often underfits the context set, possibly because the mean-aggregation step that summarizes the context set into a finite-dimensional vector acts as a bottleneck. Ideally, the closer a context point is to a target point, the more it should contribute towards the target point's prediction.\n", "\n", "    <img src=\"images/attentive-neural-processes/figure-1.png\" alt=\"Drawing\" style=\"width: 70%;\"/>\n", "\n", "* While a Gaussian Process uses a parametric kernel to quantify the notion of similarity between training points and test points, an Attentive Neural Process (ANP) employs a differentiable attention mechanism, allowing the model to learn to attend to the context points relevant to the target points. Attention has been widely used in various contexts, such as neural machine translation ([Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473), [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)) and image modeling ([Parmar et al., 2018](https://arxiv.org/abs/1802.05751)).\n", "\n", "### Methodology\n", "\n", "* Instead of replacing the mean-aggregation step in Neural Processes ([Garnelo et al., 2018](https://arxiv.org/abs/1807.01622)) with an attention layer, the authors include both of them in the model via two separate paths: a latent path using a global latent variable and a deterministic path using cross-attention. More specifically, the conditional distribution of the target points $(\\mathbf{X}_T, \\mathbf{Y}_T)$ given the context points $(\\mathbf{X}_C, \\mathbf{Y}_C)$ is specified in terms of a global latent variable $\\mathbf{z}$ and a finite-dimensional representation $\\mathbf{r}_C$ of the context sets:\n", "\n", "    $$p(\\mathbf{Y}_T \\lvert \\mathbf{X}_T, \\mathbf{X}_C, \\mathbf{Y}_C) = \\int p(\\mathbf{Y}_T \\lvert \\mathbf{X}_T, \\mathbf{r}_C, \\mathbf{z}) \\, p(\\mathbf{z} \\lvert  \\mathbf{X}_C, \\mathbf{Y}_C) \\, d\\mathbf{z}.$$\n", "\n", "    <img src=\"images/attentive-neural-processes/figure-2.png\" alt=\"Drawing\" style=\"width: 95%;\"/>\n", "\n", "* As in NPs, the global latent variable $\\mathbf{z}$ is sampled from a variational distribution that depends only on the average hidden representation of the context points. The attention-based representation $r_C$ takes the context points into account. For example, using dot-product attention:\n", "\n", "    $$\\mathbf{r}_T = \\text{Softmax} \\left(\\frac{h(\\mathbf{X}_T) h(\\mathbf{X}_C)^{\\mathsf{T}}}{\\sqrt{d_k}}  \\right) \\phi(\\mathbf{X}_C, \\mathbf{Y}_C),$$\n", "\n", "    where $h$ and $\\phi$ are some mappings parametrized as neural networks and $d_k$ is some fixed normalizing constant. The mappings $\\phi$ can also include self-attention to model the interactions between the context points. The authors mention not including attention in the latent path in order to preserve the global structure of the stochastic process.\n", "\n", "* Training proceeds via variational inference, where the global latent variable $\\mathbf{z}$ is assumed an isotropic Gaussian prior $q(\\mathbf{z} \\lvert \\mathbf{X}_C, \\mathbf{Y}_C)$ and and isotropic Gaussian posterior $q(\\mathbf{z} \\lvert \\mathbf{X}_T, \\mathbf{Y}_T, \\mathbf{X}_C, \\mathbf{Y}_C)$.\n", "\n", "* Similar to NPs, ANPs do not necessarily satisfy the consistency condition that requires the collection of joint distributions to be invariant under marginalization. However, the invariance under permutations is met as both the mean-aggregation step and the attention step don't depend on the orderings of context points.\n", "\n", "### Related Works\n", "\n", "* ANPs is related to GPs as other neural process models ([Garnelo et al., 2018](https://arxiv.org/abs/1807.01613), [Garnelo et al., 2018](https://arxiv.org/abs/1807.01622)). Another related approach inspired by Gaussian Processes is Variational Implicit Processes ([Ma et al., 2018](https://arxiv.org/abs/1806.02390)), where both the process and its posterior are approximated by a GP.\n", "\n", "### Experiments\n", "\n", "* The authors run experiments on 1D function regression to demonstrate that equipped with attention mechanism, ANPs has larger capacity to fit the training data, i.e. lower reconstruction error and lower negative log likelihood, especially those with dot-product attention and multihead attention. Experiments on 2D function regression also confirm these findings.\n", "\n", "    <img src=\"images/attentive-neural-processes/figure-3.png\" alt=\"Drawing\" style=\"width: 70%;\"/>\n", "\n", "### Thoughts\n", "\n", "* ANPs is more expressive than similar models in the same family, hence its outperformance is not surprising, especially when attention has been successfully incorporated into previous models. In that sense, the novelty of the paper is somewhat limited.\n", "\n", "### Implementation\n", "\n", "The following implementation is based on the official implementation [deepmind/neural-processes](https://github.com/deepmind/neural-processes). We first attempt to replicate the results for 1D function regression and benchmark against results produced by a Gaussian Process."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.distributions as D\n", "import torch.nn.functional as F\n", "\n", "import torchvision\n", "import torchvision.datasets as datasets\n", "import torchvision.transforms as transforms\n", "\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import random\n", "from tqdm.notebook import tqdm\n", "\n", "\n", "def gaussian_kernel(X1, X2, l1=0.6, sigma=1.0):\n", "    return sigma ** 2 * ((-0.5 / l1 ** 2) * (torch.cdist(X1, X2, p=2) ** 2)).exp()\n", "\n", "\n", "def sample_gaussian_process(inputs, l1=0.6, sigma=1.0, eps=2e-2):\n", "    # Compute covariance matrix of size B x M x M induced by a Gaussian kernel\n", "    batch_size, num_points, x_dim = inputs.size()\n", "    covariance = gaussian_kernel(inputs, inputs, l1=l1, sigma=sigma)\n", "\n", "    # Add diagonal noise to avoid singular covariance matrix\n", "    covariance += (eps ** 2) * torch.eye(num_points)\n", "    cholesky = torch.cholesky(covariance.double()).float()\n", "    outputs = torch.matmul(cholesky, torch.randn(batch_size, num_points, 1))\n", "    return outputs\n", "\n", "\n", "def generate_train_data(batch_size=16, max_num_context=50):\n", "    num_context = torch.randint(low=3, high=max_num_context, size=())\n", "    num_target = torch.randint(low=2, high=max_num_context, size=())\n", "    num_points = num_target + num_context\n", "\n", "    x_values = 4 * torch.rand(batch_size, num_points, 1) - 2\n", "    y_values = sample_gaussian_process(x_values)\n", "    x_target, y_target = x_values, y_values\n", "    x_context, y_context = x_values[:, :num_context], y_values[:, :num_context]\n", "    return x_context, y_context, x_target, y_target\n", "\n", "\n", "def generate_test_data(batch_size=1, max_num_context=50, num_target=400):\n", "    num_context = torch.randint(low=3, high=max_num_context, size=())\n", "    x_values = torch.linspace(-2, 2, 400).view(1, 400, 1).repeat((batch_size, 1, 1))\n", "    y_values = sample_gaussian_process(x_values)\n", "    x_target, y_target = x_values, y_values\n", "    idx = torch.randperm(num_target)[:num_context]\n", "    x_context, y_context = x_values[:, idx, :], y_values[:, idx, :]\n", "    return x_context, y_context, x_target, y_target\n", "\n", "\n", "def posterior_predictive(x_target, x_context, y_context, l1=0.4, sigma=1.0, eps=2e-3):\n", "    K = gaussian_kernel(x_context, x_context, l1=l1, sigma=sigma) + (eps ** 2) * torch.eye(len(x_context))\n", "    K_s = gaussian_kernel(x_context, x_target, l1=l1, sigma=sigma)\n", "    K_ss = gaussian_kernel(x_target, x_target, l1=l1, sigma=sigma)\n", "    K_inv = torch.inverse(K)\n", "\n", "    mu = K_s.transpose(1, 2).bmm(K_inv).bmm(y_context)\n", "    cov = K_ss - K_s.transpose(1, 2).bmm(K_inv).bmm(K_s)\n", "    std = (cov.diagonal(dim1=1, dim2=2) + 1e-6).sqrt()\n", "    return mu, std"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class UniformAttention(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "\n", "    def forward(self, query, key, value):\n", "        return value.mean(dim=1, keepdims=True).repeat(1, query.size(1), 1)\n", "\n", "\n", "class LaplaceAttention(nn.Module):\n", "    def __init__(self, input_dim, hidden_dim=128, scale=1., normalize=True):\n", "        super().__init__()\n", "        self.scale = scale\n", "        self.normalize = normalize\n", "        self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n", "\n", "    def forward(self, query, key, value):\n", "        query, key = self.net(query), self.net(key)\n", "        weights = -((query[:, :, None, :] - key[:, None, :, :]).abs() / self.scale).sum(dim=-1)\n", "        weights = F.softmax(weights, dim=-1) if self.normalize else 1 + F.tanh(weights)\n", "        return torch.bmm(weights, value)\n", "\n", "\n", "class DotProductAttention(nn.Module):\n", "    def __init__(self, input_dim, hidden_dim=128, normalize=True):\n", "        super().__init__()\n", "        self.normalize = normalize\n", "        self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n", "\n", "    def forward(self, query, key, value):\n", "        query, key = self.net(query), self.net(key)\n", "        weights = torch.bmm(query, key.transpose(1, 2)) / (query.size(-1) ** 0.5)\n", "        weights = F.softmax(weights, dim=-1) if self.normalize else F.sigmoid(weights)\n", "        return torch.bmm(weights, value)\n", "\n", "\n", "class MultiheadAttention(nn.Module):\n", "    def __init__(self, input_dim, output_dim, hidden_dim=128, num_heads=8):\n", "        super().__init__()\n", "        self.num_heads = num_heads\n", "        self.head_dim = output_dim // num_heads\n", "\n", "        self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n", "        self.query_proj = nn.Linear(input_dim, output_dim, bias=False)\n", "        self.key_proj = nn.Linear(input_dim, output_dim, bias=False)\n", "        self.value_proj = nn.Linear(output_dim, output_dim, bias=False)\n", "        self.output_proj = nn.Linear(output_dim, output_dim, bias=False)\n", "\n", "        for layer in [self.query_proj, self.key_proj, self.value_proj, self.output_proj]:\n", "            layer.weight.data.normal_(std=layer.in_features ** -0.5)\n", "\n", "    def forward(self, query, key, value):\n", "        batch_size, query_len, _ = query.size()\n", "        query, key = self.net(query), self.net(key)\n", "\n", "        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n", "        key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n", "        weights = torch.bmm(query, key.transpose(1, 2)) / (query.size(-1) ** 0.5)\n", "        weights = F.softmax(weights, dim=-1)\n", "\n", "        value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n", "        output = torch.bmm(weights, value).view(batch_size, self.num_heads, query_len, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, query_len, self.num_heads * self.head_dim)\n", "        return self.output_proj(output)\n", "\n", "\n", "class LinearNormal(nn.Module):\n", "    def __init__(self, in_features, out_features):\n", "        super().__init__()\n", "        self.linear = nn.Linear(in_features, 2 * out_features)\n", "\n", "    def forward(self, inputs):\n", "        mean, std = self.linear(inputs).chunk(2, dim=-1)\n", "        std = 0.1 + 0.9 * F.softplus(std)\n", "        return D.Normal(loc=mean, scale=std)\n", "\n", "\n", "class AttentiveNeuralProcess(nn.Module):\n", "    def __init__(self, hidden_dim=128, x_dim=1, y_dim=1, z_dim=128):\n", "        super().__init__()\n", "        self.attention_encoder = nn.Sequential(nn.Linear(x_dim + y_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim),)\n", "        self.attention_mapping = DotProductAttention(x_dim, hidden_dim)\n", "        self.latent_encoder = nn.Sequential(nn.Linear(x_dim + y_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim),)\n", "        self.latent_mapping = LinearNormal(hidden_dim, z_dim)\n", "        self.decoder = nn.Sequential(nn.Linear(2 * hidden_dim + x_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), LinearNormal(hidden_dim, y_dim),)\n", "\n", "    def forward(self, x_context, y_context, x_target, y_target=None):\n", "        # x_context: [batch_size, num_context, x_dim]\n", "        # y_context: [batch_size, num_context, y_dim]\n", "        # x_target: [batch_size, num_points, x_dim]\n", "        # y_target: [batch_size, num_points, y_dim]\n", "\n", "        # Compute attentive representations for target outputs\n", "        h_context = self.attention_encoder(torch.cat([x_context, y_context], dim=-1))\n", "        h_target = self.attention_mapping(x_target, x_context, h_context)\n", "\n", "        # Infer a global latent variable based on the context input-output pairs and\n", "        # map the global latent variable to a factorized Gaussian distribution\n", "        r_context = self.latent_encoder(torch.cat([x_context, y_context], dim=-1)).mean(dim=1)\n", "        z_prior = self.latent_mapping(r_context)\n", "\n", "        # After training, target input-output pairs are not available. We concatenate the deterministic\n", "        # attention representation and a sample from z_prior to each target input to make predictions.\n", "        if y_target is None:\n", "            z_sample = z_prior.rsample()[:, None, :].repeat(1, x_target.size(1), 1)\n", "            y_pred = self.decoder(torch.cat([h_target, z_sample, x_target], dim=-1))\n", "            return y_pred\n", "\n", "        # During training, infer another factorized Gaussian distribution using the\n", "        # target input-output pairs, later to be matched with the previous distribution\n", "        r_target = self.latent_encoder(torch.cat([x_target, y_target], dim=-1)).mean(dim=1)\n", "        z_posterior = self.latent_mapping(r_target)\n", "\n", "        # Concatenate a sample from z_posterior to each target input to make predictions\n", "        z_sample = z_posterior.rsample()[:, None, :].repeat(1, x_target.size(1), 1)\n", "        y_pred = self.decoder(torch.cat([h_target, z_sample, x_target], dim=-1))\n", "        return z_prior, z_posterior, y_pred\n", "\n", "\n", "model = AttentiveNeuralProcess().cuda()\n", "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n", "progress_bar = tqdm(range(int(1e5 + 1)), desc=\"Iteration\", leave=False)\n", "\n", "for it in progress_bar:\n", "    x_context, y_context, x_target, y_target = generate_train_data(max_num_context=50)\n", "    z_prior, z_posterior, y_pred = model(x_context.cuda(), y_context.cuda(), x_target.cuda(), y_target.cuda())\n", "\n", "    log_prob = y_pred.log_prob(y_target.cuda()).mean(dim=0).sum()\n", "    kl_loss = D.kl_divergence(z_posterior, z_prior).mean(dim=0).sum()\n", "    loss = -log_prob + kl_loss\n", "\n", "    model.zero_grad()\n", "    loss.backward()\n", "    optimizer.step()\n", "    progress_bar.set_postfix({\"loss\": \"{:.3f}\".format(loss.item())})\n", "\n", "    if it % 20000 == 0:\n", "        x_context, y_context, x_target, y_target = generate_test_data(max_num_context=10)\n", "        y_pred = model(x_context.cuda(), y_context.cuda(), x_target.cuda())\n", "        mu, sigma = y_pred.loc.detach().cpu().numpy(), y_pred.scale.detach().cpu().numpy()\n", "\n", "        fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n", "        ax.plot(x_target[0], mu[0], c=\"C0\", label=\"NP\")\n", "        ax.scatter(x_context[0], y_context[0], c=\"r\", s=15, label=\"Context\")\n", "        ax.plot(x_target[0], y_target[0], c=\"C2\", label=\"Truth\")\n", "        ax.set_title(\"Iteration {}: loss {:.3f}\".format(it, loss.item()))\n", "        ax.fill_between(x_target[0, :, 0], mu[0, :, 0] - sigma[0, :, 0], mu[0, :, 0] + sigma[0, :, 0], alpha=0.2, facecolor='C0', interpolate=True)\n", "\n", "        mu, std = posterior_predictive(x_target, x_context, y_context)\n", "        ax.plot(x_target[0], mu[0], c='C1', label=\"GP\")\n", "        ax.fill_between(x_target[0].squeeze(), mu[0].squeeze() - std[0], mu[0].squeeze() + std[0], facecolor='C1', alpha=0.2)\n", "        ax.legend()\n", "        plt.show()"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "source": ["from IPython.core.display import HTML\n", "HTML(open('../css/custom.css', 'r').read())"], "outputs": [{"data": {"text/html": ["<style>\n", "    @font-face {\n", "        font-family: \"Roboto\";\n", "        font-weight: normal;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/roboto/roboto-regular.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"Roboto\";\n", "        font-weight: normal;\n", "        font-style: italic;\n", "        src:url(\"../css/fonts/roboto/roboto-italic.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"Roboto\";\n", "        font-weight: bold;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/roboto/roboto-bold.ttf\") format(\"truetype\");\n", "    }\n", "\n", "    @font-face {\n", "        font-family: \"Consolas\";\n", "        font-weight: normal;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/consolas/consolas-regular.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"Consolas\";\n", "        font-weight: normal;\n", "        font-style: italic;\n", "        src:url(\"../css/fonts/consolas/consolas-italic.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"Consolas\";\n", "        font-weight: bold;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/consolas/consolas-bold.ttf\") format(\"truetype\");\n", "    }\n", "\n", "    @font-face {\n", "        font-family: \"SF Mono\";\n", "        font-weight: normal;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/sfmono/sfmono-regular.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"SF Mono\";\n", "        font-weight: normal;\n", "        font-style: italic;\n", "        src:url(\"../css/fonts/sfmono/sfmono-italic.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"SF Mono\";\n", "        font-weight: bold;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/sfmono/sfmono-bold.ttf\") format(\"truetype\");\n", "    }\n", "\n", "    @font-face {\n", "        font-family: \"CMU Sans Serif\";\n", "        font-weight: normal;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/cmu/cmu-regular.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"CMU Sans Serif\";\n", "        font-weight: bold;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/cmu/cmu-bold.ttf\") format(\"truetype\");\n", "    }\n", "\n", "    /* Change text font */\n", "    div.text_cell_render {\n", "        font-family: CMU Sans Serif, monospace;\n", "        font-size: 16px;\n", "        -ms-text-size-adjust: 100%;\n", "        -webkit-text-size-adjust: 100%;\n", "        color: #24292e;\n", "        line-height: 1.3;\n", "    }\n", "\n", "    /* Change code font */\n", "    .CodeMirror pre {\n", "        font-family: SF Mono, monospace;\n", "        padding-left: 8px;\n", "        padding-right: 8px;\n", "        font-size: 12px;\n", "        line-height: 130%;\n", "    }\n", "\n", "    .CodeMirror-linenumber {\n", "        background: #fff;\n", "        font-family: SF Mono, monospace;\n", "        font-size: 11.5px;\n", "    }\n", "\n", "    /* Font changes for output needed for tqdm progress bar */\n", "    div.output_area pre {\n", "        font-family: SF Mono, monospace;\n", "        font-size: 11.5px;\n", "        padding-top: 5px;\n", "        padding-bottom: 5px;\n", "    }\n", "\n", "    div.output_subarea {\n", "        padding-top: 0px;\n", "        padding-bottom: 0px;\n", "        border: 0px;\n", "    }\n", "\n", "    /* Center plots */\n", "    .output_png {\n", "        display: table-cell;\n", "        text-align: center;\n", "        vertical-align: middle;\n", "    }\n", "\n", "    /* Font changes for widgets, useful for tqdm progress bar */\n", "    label.widget-label {\n", "        font-size: 12px;\n", "    }\n", "\n", "    div.p-Widget.jupyter-widgets.widget-inline-hbox.widget-html {\n", "        font-size: 12px;\n", "    }\n", "\n", "    div.jupyter-widgets-view {\n", "        font-family: SF Mono, monospace;\n", "    }\n", "\n", "    div.bk-root, div.bk-plot-wrapper, div.bk-canvas-overlays {\n", "        font-family: SF Mono, monospace;\n", "    }\n", "\n", "    div.bk-canvas-events {\n", "        display: none;\n", "    }\n", "\n", "    /* Syntax highlighting */\n", "    .cm-s-ipython span.cm-keyword {\n", "        color: #B43673;\n", "        font-weight: normal;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-number {\n", "        color: #3482f5;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-string {\n", "        color: #479035;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-def {\n", "        color: #000;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-builtin {\n", "        color: #790EAD;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-operator {\n", "        color: #B43673;\n", "        font-weight: normal;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-comment {\n", "        font-style: normal;\n", "        color: #927E41;\n", "    }\n", "\n", "    .cm-s-ipython div.CodeMirror-selected {\n", "        background: #D4E3FC !important;\n", "    }\n", "\n", "    /* Change debugging colors */\n", "    .ansi-green-fg {\n", "        color: #479035;\n", "    }\n", "\n", "    .ansi-cyan-fg {\n", "        color: #3482f5;\n", "    }\n", "\n", "    .ansi-cyan-fg {\n", "        color: #000;\n", "    }\n", "\n", "    .ansi-red-fg {\n", "        color: #c74230;\n", "    }\n", "\n", "    /* Change code font */\n", "    .rendered_html code {\n", "        font-family: SF Mono, monospace;\n", "    }\n", "\n", "    /* Disable prompt */\n", "    .prompt{\n", "        display: None;\n", "    }\n", "\n", "    /* Change font for headers */\n", "    div.text_cell_render h1,\n", "    div.text_cell_render h2,\n", "    div.text_cell_render h3,\n", "    div.text_cell_render h4,\n", "    div.text_cell_render h5,\n", "    div.text_cell_render h6 {\n", "        font-weight: 300;\n", "    }\n", "\n", "    div.text_cell_render h1 {\n", "        font-size: 20pt;\n", "    }\n", "\n", "    div.text_cell_render h2 {\n", "        font-size: 17pt;\n", "    }\n", "\n", "    div.text_cell_render h3 {\n", "        font-size: 16pt;\n", "    }\n", "\n", "    /* Changes for nbviewer */\n", "    div.container.container-main {\n", "        margin-right: auto;\n", "        margin-left: auto;\n", "        color:#333;\n", "        background:#fff;\n", "\n", "        width: 980px !important;\n", "        padding: 45px !important;\n", "        border: 1px solid #ddd;\n", "        border-radius: 3px;\n", "        word-wrap: break-word;\n", "    }\n", "    code, pre {\n", "        font-family: SF Mono, monospace !important;\n", "        padding-left: 8px !important;\n", "        padding-right: 8px !important;\n", "        font-size: 12px !important;\n", "        line-height: 140% !important;\n", "        overflow: none;\n", "        overflow-x: auto;\n", "    }\n", "    body {\n", "        margin-top: 0px;\n", "        padding-top: 0px;\n", "    }\n", "    .navbar, footer, .breadcrumb {\n", "        display: none !important;\n", "    }\n", "    .highlight .k, .highlight .bp, .highlight .kn, .highlight .kc, .highlight .o, .highlight .ow {\n", "        color: #B43673 !important;\n", "        font-weight: normal !important;\n", "    }\n", "    .highlight .mi, .highlight .mf {\n", "        color: #3482f5 !important;\n", "        font-weight: normal !important;\n", "    }\n", "    .highlight .nn, .highlight .nc, .highlight .nf, .highlight .p, .highlight .n, .highlight .sa {\n", "        color: #000 !important;\n", "        font-weight: normal !important;\n", "    }\n", "    .highlight .s1, .highlight .s2, .highlight .si, .highlight .sd {\n", "        color: #479035 !important;\n", "        font-weight: normal !important;\n", "        font-style: normal !important;\n", "    }\n", "    .highlight .nb {\n", "        color: #790EAD !important;\n", "    }\n", "    .highlight .c1 {\n", "        color: #927E41 !important;\n", "        font-style: normal !important;\n", "    }\n", "</style>\n", "\n", "\n", "<script>\n", "    MathJax.Hub.Config({\n", "        TeX: {\n", "            extensions: [\"AMSmath.js\"],\n", "            equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n", "        },\n", "        tex2jax: {\n", "            inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n", "            displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n", "            processEscapes: true,\n", "            processEnvironments: true\n", "        },\n", "        MathML: {\n", "            extensions: ['content-mathml.js']\n", "        },\n", "        \"HTML-CSS\": {\n", "            availableFonts: [\"TeX\"],\n", "            imageFont: null,\n", "            preferredFont: \"TeX\",\n", "            webFont: \"TeX\",\n", "            styles: {'.MathJax_Display': {\"margin\": \"10px\"}},\n", "            linebreaks: { automatic: true }\n", "        },\n", "    });\n", "    MathJax.Hub.Queue(\n", "        [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n", "        [\"PreProcess\", MathJax.Hub],\n", "        [\"Reprocess\", MathJax.Hub]\n", "    );\n", "</script>\n"], "text/plain": ["<IPython.core.display.HTML object>"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 2}