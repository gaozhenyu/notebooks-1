{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax Games\n",
    "\n",
    "Consider a two-player game $\\min_{\\mathbf{x}} \\max_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y})$ where $f: \\mathbb{R}^m \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is twice continuously differentiable. An interesting example is generative adversarial networks (GANs):\n",
    "\n",
    "\\\\[ \\min_{G} \\max_{D} \\, \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[ \\log(D(x)) \\right] + \\mathbb{E}_{z \\sim p_{\\text{latent}}} \\left[ \\log(1 - D(G(z))) \\right]. \\\\]\n",
    "\n",
    "Here $\\mathbf{x}$ represents the parameters of the generator $G$ which aims to transform latent vectors $z \\sim p_{\\text{latent}}$ into samples $G(z)$ that mimic those from a given data distribution $p_{\\text{data}}$, and $\\mathbf{y}$ represents the parameters of the discriminator $D$ which minimizes a [log loss](http://wiki.fast.ai/index.php/Log_Loss) to differentiate between $G(z)$ and $x \\sim p_{\\text{data}}$. As in the case of GANs, the players of minimax games are often parametrized by neural networks, and the loss functions $f(\\mathbf{x}, \\mathbf{y})$ are not necessarily convex in $\\mathbf{x}$ or concave in $\\mathbf{y}$. Hence, Nash equilibria do not necessarily exist, and even if they do, finding a global Nash equilibrium is hopelessly impractical. We therefore restrict to gradient-based methods that hopefully converge to local solutions.\n",
    "\n",
    "For convenience, let $\\mathbf{w} = (\\mathbf{x}, \\mathbf{y}) \\in \\mathbb{R}^{m + n}$ denote the vector of combined parameters and $\\xi(\\mathbf{w}) = \\xi(\\mathbf{x}, \\mathbf{y}) =(\\nabla_{\\mathbf{x}} f(\\mathbf{w}), -\\nabla_{\\mathbf{y}}f(\\mathbf{w})) \\in \\mathbb{R}^{m + n}$ denote the signed vector of partial derivatives, often known as the simultaneous gradient. The Hessian of the game is a $(m+ n) \\times (m + n)$-matrix of second-order derivaties, which is not necessarily symmetric:\n",
    "\n",
    "\\\\[ \\mathbf{H}(\\mathbf{w}) := \\nabla_{\\mathbf{w}} \\cdot \\xi(\\mathbf{w})^{\\mathsf{T}} = \\begin{bmatrix}\\nabla^2_{\\mathbf{x}\\mathbf{x}} f(\\mathbf{w}) & \\nabla^2_{\\mathbf{x}\\mathbf{y}} f(\\mathbf{w}) \\\\  -\\nabla^2_{\\mathbf{y}\\mathbf{x}} f(\\mathbf{w}) & -\\nabla^2_{\\mathbf{y}\\mathbf{y}} f(\\mathbf{w}) \\end{bmatrix}. \\\\]\n",
    "\n",
    "**Definition 1:** A point $\\mathbf{w}^\\star = (\\mathbf{x}^\\star, \\mathbf{y}^\\star)$ is a **local minimax** (or Nash equilibirum) if there is a neighborhood $U$ of $(\\mathbf{x}^\\star, \\mathbf{y}^\\star)$ such that $f(\\mathbf{x}^\\star, \\mathbf{y}) \\leq f(\\mathbf{x}^\\star, \\mathbf{y}^\\star) \\leq f(\\mathbf{x}, \\mathbf{y}^\\star)$ for all $(\\mathbf{x}, \\mathbf{y}) \\in U$. These conditions are equivalent to $\\xi(\\mathbf{w}^\\star) = 0$ and $\\nabla^2_{\\mathbf{x}\\mathbf{x}} f(\\mathbf{w}^\\star) \\succeq 0$ and $\\nabla^2_{\\mathbf{y}\\mathbf{y}} f(\\mathbf{w}^\\star) \\preceq 0$.\n",
    "\n",
    "**Definition 2:** A point $\\mathbf{w}$ in a discrete time dynamical system with update rule $\\mathbf{w}_{t + 1} = \\omega(\\mathbf{w}_{t})$ is called a **fixed point** if $\\omega(\\mathbf{w}) = \\mathbf{w}$. A fixed point $\\mathbf{w}$ is **stable** if the spectral radius $\\rho(\\mathbf{J}(\\mathbf{w}))$ is at most 1, where $\\mathbf{J(\\mathbf{w})}$ is the Jacobian of $\\omega$ computed at $\\mathbf{w}$.\n",
    "\n",
    "The reason we're interested in spectral analysis of the Jacobian of the fixed points is the following well-known fact: if a fixed point $\\mathbf{w}$ is stable and hyperbolic (i.e. $\\mathbf{J}(\\mathbf{w})$ has no eigenvalues with absolute value 1), there is a small neighborhood around $\\mathbf{w}$ such that all initializations in that neighborhood results in convergence to $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Ascent (GDA)\n",
    "A straightforward optimization routine to solve $\\min_{\\mathbf{x}} \\max_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y})$ is <em>gradient descent-ascent</em> (GDA), where both players take a gradient update simultaneously $\\mathbf{w}_{t + 1} = \\mathbf{w}_t - \\eta \\xi(\\mathbf{w})$, i.e.:\n",
    "\n",
    "\\\\[ \\begin{bmatrix}\\mathbf{x}_{t + 1} \\\\ \\mathbf{y}_{t + 1} \\end{bmatrix} = \\begin{bmatrix}\\mathbf{x}_{t} \\\\ \\mathbf{y}_{t} \\end{bmatrix} - \\eta \\begin{bmatrix}\\nabla_{\\mathbf{x}} f( \\mathbf{x}_t, \\mathbf{y}_t)  \\\\ -\\nabla_{\\mathbf{y}} f( \\mathbf{x}_t, \\mathbf{y}_t) \\end{bmatrix}. \\\\]\n",
    "\n",
    "**Theorem 1** ([Mescheder et al., 2017](http://papers.nips.cc/paper/6779-the-numerics-of-gans.pdf), [Daskalakis and Panageas (2018](https://papers.nips.cc/paper/8136-the-limit-points-of-optimistic-gradient-descent-in-min-max-optimization.pdf)): If the Hessian computed at a local minimax has no purely imaginary eigenvalue, then the local minimax is a stable fixed point with small enough learning rate.\n",
    "\n",
    "**Proof of Theorem 1**: \n",
    "Before we prove Theorem 1, it's worth noting that stability does not guarantee local minimaxity. For example, $(0, 0)$ is not a local minimax of $f(x, y) = 3x^2 + 4xy + y^2$ due to $\\nabla_{\\mathbf{y}\\mathbf{y}}((0, 0)) = 2 > 0$, though it is a stable fixed point of GDA for all $\\eta < 1$ (the Jacobian of GDA has eigenvalues $1 - 2\\eta$). \n",
    "\n",
    "More generally, the Jacobian of GDA at a local minimax $\\mathbf{w}^\\star$ is $\\mathbf{J}(\\mathbf{w}^\\star) = \\mathbf{I} - \\eta \\mathbf{H}(\\mathbf{w}^\\star)$, which has eigenvalues $1 - \\eta \\lambda(\\mathbf{H})$ where $\\lambda(\\mathbf{H})$ are eigenvalues of the Hessian evaluated at $\\mathbf{w}^\\star$. Since $\\mathbf{w}^\\star$ is a local minimax, $\\nabla^2_{\\mathbf{x}\\mathbf{x}}(\\mathbf{w}^\\star)$ and $\\nabla^2_{\\mathbf{y}\\mathbf{y}}(\\mathbf{w}^\\star)$ are positive semidefinite and thus by Ky Fan inequality, $\\text{Re}(\\lambda(\\mathbf{H})) \\geq \\frac{1}{2} \\lambda_\\min(\\mathbf{H} + \\mathbf{H}^\\mathsf{T}) \\geq 0$. By choosing $\\eta < 2\\min_{\\lambda(\\mathbf{H})} \\left\\{\\text{Re}(\\lambda(\\mathbf{H})) / |\\lambda(\\mathbf{H})|^2  \\right\\}$, we have $|1 - \\eta \\lambda(\\mathbf{H})| = 1 - \\eta (2\\text{Re}(\\lambda(\\mathbf{H}) - \\eta |\\lambda(\\mathbf{H})|^2) < 1$. In other words, any local minimax $\\mathbf{w}^\\star$ of GDA is stable if the learning rate $\\eta$ is small enough. However, if the Hessian has an eigenvalue with a small real part but a large imaginary part ($\\text{Re}(\\lambda(\\mathbf{H})) / |\\lambda(\\mathbf{H})|^2$ is small), the learning rate has to be very small, which implies extremely slow convergence.\n",
    "\n",
    "In case the Hessians contain purely imaginary eigenvalues, Theorem 1 does not guarantee convergence of GDA. In fact, recent works ([Mertikopoulos et al., 2018](https://arxiv.org/abs/1709.02738), [Balduzzi et al., 2018](https://arxiv.org/abs/1802.05642)) show that GDA exhibits strong rotation around fixed points and sometimes diverges. In the simple bilinear setting where $f(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^{\\mathsf{T}} \\mathbf{A} \\mathbf{y}$ for some matrix $\\mathbf{A}$, for example, the simultaneous gradient $\\xi(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{A} \\mathbf{y}, - \\mathbf{A}^{\\mathsf{T}} \\mathbf{x})$ implies\n",
    "\n",
    "\\\\[ \\mathbf{w}_{t + 1} = \\begin{bmatrix}\\mathbf{I} & -\\mathbf{A} \\\\ \\mathbf{A}^{\\mathsf{T}} & \\mathbf{I}  \\end{bmatrix}\\mathbf{w}_{t} = \\det(\\mathbf{I} + \\mathbf{A} \\mathbf{A}^{\\mathsf{T}}) \\, (\\mathbf{R} \\mathbf{w}) \\quad\\qquad \\text{where} \\quad\\qquad \\mathbf{R} = \\frac{1}{\\det(\\mathbf{I} + \\mathbf{A} \\mathbf{A}^{\\mathsf{T}})}  \\begin{bmatrix}\\mathbf{I} & -\\mathbf{A} \\\\ \\mathbf{A}^{\\mathsf{T}} & \\mathbf{I}  \\end{bmatrix} \\in \\text{SO}(m + n) .\\\\]\n",
    "\n",
    "is a rotation matrix. As a result, GDA updates show cyclic behavior around $(\\mathbf{0}, \\mathbf{0})$ and can easily diverge when $\\det(\\mathbf{I} + \\mathbf{A} \\mathbf{A}^{\\mathsf{T}}) > 1$. The rotation phenomenon is not limited to the bilinear setting (see Figure 1); it is commonly observed that the generator in GANs often cycles through a subset of modes and fails to capture the diversity of the data distribution (see Figure 2), a problem often known as mode collapsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/mechanics-of-differentiable-games/softplus.png\" style='margin: 10px auto' alt=\"my alt text\"/>\n",
    "  <figcaption>Figure 1: The paths taken by various gradient-based methods next to a loss surface and its contour for solving $\\min_x \\max_y f(x, y)$ where $f(x, y) = \\log(1 + e^x) + 3xy - \\log(1 + e^y)$.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/mechanics-of-differentiable-games/gda.gif\" style='margin: 10px auto' alt=\"my alt text\"/>\n",
    "  <figcaption>Figure 2: Training GAN on a mixture of 16 Gaussians with gradient descent ascent (GDA). Left: Kernel density plot of samples generated by the generator. Middle: Scatter plots of generated samples in orange and true samples in blue together with contours of the discriminator. Right: Training loss values of the generator and the discrimnator. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consensus Optimization (CO)\n",
    " \n",
    "[Mescheder et al. (2017)](http://papers.nips.cc/paper/6779-the-numerics-of-gans.pdf) observed that when the rotation phenomenon happens, simultaneous gradients $\\xi(\\mathbf{x}, \\mathbf{y})$ decrease slowly in norm (consider the bilinear setting where $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}$ and $\\mathbf{A} = 1$). Since $\\xi(\\mathbf{x}, \\mathbf{y})$ at fixed points are 0, one way to mitigate rotation is to directly penalize $\\|\\xi(\\mathbf{x}, \\mathbf{y})\\|^2$, i.e. solving $\\min_{\\mathbf{x}} \\ell_1(\\mathbf{x}, \\mathbf{y})$ and $\\min_{\\mathbf{y}} \\ell_2(\\mathbf{x}, \\mathbf{y})$ simultaneously where\n",
    "\n",
    "\\\\[ \\ell_1(\\mathbf{x}, \\mathbf{y}) = f(\\mathbf{x}, \\mathbf{y}) + \\frac{1}{2}\\gamma \\|\\xi(\\mathbf{x}, \\mathbf{y})\\|^2, \\qquad \\ell_2(\\mathbf{x}, \\mathbf{y}) = -f(\\mathbf{x}, \\mathbf{y}) + \\frac{1}{2}\\gamma \\|\\xi(\\mathbf{x}, \\mathbf{y})\\|^2 \\\\]\n",
    "\n",
    "and $\\gamma > 0$ is a hyperparameter. The new optimization method is called consensus optimization (CO), whose gradient update has the form $\\mathbf{w}_{t + 1} = \\mathbf{w}_t - \\eta \\xi(\\mathbf{w}) - \\eta \\gamma \\mathbf{H}^{\\mathsf{T}}(\\mathbf{w}) \\xi(\\mathbf{w})$, i.e.\n",
    "\n",
    "\\\\[ \\begin{bmatrix}\\mathbf{x}_{t + 1} \\\\ \\mathbf{y}_{t + 1} \\end{bmatrix} = \\begin{bmatrix}\\mathbf{x}_{t} \\\\ \\mathbf{y}_{t} \\end{bmatrix} - \\eta \\begin{bmatrix}\\nabla_{\\mathbf{x}} f( \\mathbf{x}_t, \\mathbf{y}_t)  \\\\ -\\nabla_{\\mathbf{y}} f( \\mathbf{x}_t, \\mathbf{y}_t) \\end{bmatrix} - \\eta \\gamma \\begin{bmatrix}\\nabla^2_{\\mathbf{x}\\mathbf{x}} f(\\mathbf{x}, \\mathbf{y}) & \\nabla^2_{\\mathbf{x}\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y}) \\\\  -\\nabla^2_{\\mathbf{y}\\mathbf{x}} f(\\mathbf{x}, \\mathbf{y}) & -\\nabla^2_{\\mathbf{y}\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y}) \\end{bmatrix}^\\mathbf{\\mathsf{T}} \\begin{bmatrix}\\nabla_{\\mathbf{x}} f( \\mathbf{x}_t, \\mathbf{y}_t)  \\\\ -\\nabla_{\\mathbf{y}} f( \\mathbf{x}_t, \\mathbf{y}_t) \\end{bmatrix}. \\\\]\n",
    "\n",
    "Here, we can view CO as GDA on the new loss functions where the simultaneous gradient is $\\xi_\\gamma(\\mathbf{w}) = \\xi(\\mathbf{w}) + \\gamma \\mathbf{H}^{\\mathsf{T}}(\\mathbf{w}) \\xi(\\mathbf{w}) = (\\mathbf{I} + \\gamma \\mathbf{H}^{\\mathsf{T}} (\\mathbf{w}))\\xi(\\mathbf{w})$ and the Hessian is $\\mathbf{H}_\\gamma(\\mathbf{w}) = \\nabla_{\\mathbf{w}} \\cdot \\xi_\\gamma(\\mathbf{w})^{\\mathsf{T}} = \\mathbf{H}(\\mathbf{w}) + \\gamma \\mathbf{H}^{\\mathsf{T}}(\\mathbf{w}) \\mathbf{H}(\\mathbf{w}) + (\\nabla_{\\mathbf{w}} \\cdot \\mathbf{H}(\\mathbf{w})) \\xi(\\mathbf{w})$. Since the objective functions have changed, a natural question to ask is whether the local minimaxes of the original game $\\min_{\\mathbf{x}} \\max_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y})$ are still retained. The answer is yes, if $\\gamma$ is small enough, for two following reasons:\n",
    "* $\\xi_\\gamma(\\mathbf{w}^\\star) = 0$ implies $\\xi(\\mathbf{w}^\\star) = 0$, if we pick $\\gamma$ such that $-\\gamma^{-1}$ is not an eigenvalue of $\\mathbf{H}(\\mathbf{w}^\\star)$, i.e. $\\mathbf{I} + \\gamma \\mathbf{H}^{\\mathsf{T}} (\\mathbf{w}^\\star)$ is invertible.\n",
    "* At any local minimax $\\mathbf{w}^\\star$ of the original game, $\\xi(\\mathbf{w}^\\star) = 0$ implies that $\\mathbf{H}_\\gamma(\\mathbf{w}^\\star) = \\mathbf{H}(\\mathbf{w}^\\star) + \\gamma \\mathbf{H}^{\\mathsf{T}}(\\mathbf{w}^\\star) \\mathbf{H}(\\mathbf{w}^\\star)$. Since $\\nabla^2_{\\mathbf{x}\\mathbf{x}} f(\\mathbf{w}^\\star) \\succeq 0$ and $\\nabla^2_{\\mathbf{y}\\mathbf{y}} f(\\mathbf{w}^\\star) \\preceq 0$ and $\\mathbf{H}^{\\mathsf{T}}(\\mathbf{w}^\\star) \\mathbf{H}(\\mathbf{w}^\\star)$ is positive semi definite, it is clear that $\\nabla^2_{\\mathbf{x}\\mathbf{x}} \\ell_1 (\\mathbf{w}^\\star) \\succeq 0$ for all $\\gamma$ and $\\nabla^2_{\\mathbf{y}\\mathbf{y}} \\ell_2 (\\mathbf{w}^\\star) \\preceq 0$ for small enough $\\gamma$. Note that $\\nabla^2_{\\mathbf{y}\\mathbf{y}} \\ell_2 (\\mathbf{w}^\\star)$ is actually \"less positive definite\" than $\\nabla^2_{\\mathbf{y}\\mathbf{y}} f(\\mathbf{w}^\\star)$ because of $\\mathbf{H}^{\\mathsf{T}}(\\mathbf{w}^\\star) \\mathbf{H}(\\mathbf{w}^\\star)$.\n",
    "\n",
    "By an argument similar to the proof of Theorem 1, we can easily show that any eigenvalue $\\lambda(\\mathbf{H}_\\gamma)$ of $\\mathbf{H}_\\gamma(\\mathbf{w}^\\star)$ has nonnegative real part: $\\text{Re}(\\lambda(\\mathbf{H}_\\gamma)) \\geq \\frac{1}{2} \\lambda_{\\min} (\\mathbf{H}_\\gamma + \\mathbf{H}_\\gamma^{\\mathsf{T}}) \\geq 0$. [Mescheder et al. (2017)](http://papers.nips.cc/paper/6779-the-numerics-of-gans.pdf) also came up with some upper bound for the imaginary-to-real ratio, showing that that convergence of CO is potentially faster and more stable, although the bound is not intuitive. Empirically, consensus optimization works quite well in settings where GDA struggles with rotational forces (see Figure 1), but its performance is very sensitive to the choice of $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/mechanics-of-differentiable-games/co.gif\" style='margin: 10px auto' alt=\"my alt text\"/>\n",
    "  <figcaption>Figure 2: Training GAN on a mixture of 16 Gaussians with consensus optimization (CO). Left: Kernel density plot of samples generated by the generator. Middle: Scatter plots of generated samples in orange and true samples in blue together with contours of the discriminator. Right: Training loss values of the generator and the discrimnator. </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symplectic Gradient Adjustment (SGA)\n",
    "To be continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with Toy Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import linalg\n",
    "from IPython.core.display import HTML\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def plot_3d(ax, func, xrange, yrange, cmap=\"viridis\", elev=None, azim=None):\n",
    "    X = np.arange(xrange[0], xrange[1], 0.01)\n",
    "    Y = np.arange(yrange[0], yrange[1], 0.01)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    Z = func.fn(X, Y)\n",
    "\n",
    "    ax.plot_surface(X, Y, Z, cmap=cmap)\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.grid(False)\n",
    "    ax.dist = 7.5\n",
    "\n",
    "    ax.set_xlim(xrange)\n",
    "    ax.set_ylim(yrange)\n",
    "    ax.set_zlim(np.min(Z), np.max(Z))\n",
    "\n",
    "\n",
    "def plot_2d(ax, func, xrange=[-4.5, 4.5], yrange=[-4.5, 4.5], logz=False, num_lines=60, **kwargs):\n",
    "    X = np.arange(xrange[0], xrange[1], 0.01)\n",
    "    Y = np.arange(yrange[0], yrange[1], 0.01)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    Z = func.fn(X, Y)\n",
    "\n",
    "    Z = np.log(Z) if logz else Z\n",
    "    ax.contour(X, Y, Z, num_lines, linewidths=1.0, **kwargs)\n",
    "    ax.set_xlim(xrange)\n",
    "    ax.set_ylim(yrange)\n",
    "\n",
    "\n",
    "def plot_path(\n",
    "    func, methods, lr, init_point, min_point=None, num_steps=200, xrange=(-4.5, 4.5), yrange=(-4.5, 4.5), azim=None, elev=None, cmap=\"viridis\", num_lines=60, figsize=(14, 4), **kwargs,\n",
    "):\n",
    "    xy_values, loss_values = OrderedDict(), OrderedDict()\n",
    "    for method in methods:\n",
    "        method_fn = eval(method)\n",
    "        xy_values[method], loss_values[method] = method_fn(func, lr, init_point, num_steps, **kwargs)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "    plot_3d(ax1, func, xrange, yrange, elev=elev, azim=azim, cmap=cmap)\n",
    "    ax1.plot([min_point[0]], [min_point[1]], [func.fn(min_point[0], min_point[1])], zorder=10, marker=(5, 1), markersize=10, alpha=0.4, color=\"C3\") if min_point else None\n",
    "    for i, method in enumerate(xy_values.keys()):\n",
    "        ax1.plot(xy_values[method][:, 0], xy_values[method][:, 1], loss_values[method], \"-o\", zorder=10, markersize=1.5, linewidth=1, color=\"C%d\" % i, label=method.upper())\n",
    "\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    plot_2d(ax2, func, xrange, yrange, num_lines=num_lines, cmap=cmap)\n",
    "    ax2.plot([min_point[0]], [min_point[1]], marker=(5, 1), markersize=10, alpha=0.4, color=\"C3\") if min_point else None\n",
    "    for i, (method, xy_value) in enumerate(xy_values.items()):\n",
    "        ax2.plot(xy_value[:, 0], xy_value[:, 1], \"-o\", markersize=1.5, linewidth=1, color=\"C%d\" % i, label=method.upper())\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gda(func, lr, init_point, num_steps=100, **kwargs):\n",
    "    x, y = init_point\n",
    "    xy_values, loss_values = [], []\n",
    "    for _ in range(num_steps):\n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        x = x - lr * func.dx(x, y)\n",
    "        \n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        y = y + lr * func.dy(x, y)\n",
    "    return np.array(xy_values), np.array(loss_values)\n",
    "\n",
    "\n",
    "def sga(func, lr, init_point, num_steps=100, **kwargs):\n",
    "    x, y = init_point\n",
    "    xy_values, loss_values = [], []\n",
    "    for _ in range(num_steps):\n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        x = x - lr * (func.dx(x, y) + kwargs['lambd'] * func.dxy(x, y) * func.dy(x, y))\n",
    "        \n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        y = y + lr * (func.dy(x, y) - kwargs['lambd'] * func.dyx(x, y) * func.dx(x, y))\n",
    "    return np.array(xy_values), np.array(loss_values)\n",
    "\n",
    "\n",
    "def eg(func, lr, init_point, num_steps=100, **kwargs):\n",
    "    x, y = init_point\n",
    "    xy_values, loss_values = [], []\n",
    "    for _ in range(num_steps):\n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        x = x - lr * func.dx(x - lr * func.dx(x, y), y + lr * func.dy(x, y))\n",
    "        \n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        y = y + lr * func.dy(x - lr * func.dx(x, y), y + lr * func.dy(x, y))\n",
    "    return np.array(xy_values), np.array(loss_values)\n",
    "\n",
    "\n",
    "def ogda(func, lr, init_point, num_steps=100, **kwargs):\n",
    "    x, y = init_point\n",
    "    xy_values, loss_values = [], []\n",
    "    x_prev, y_prev = x, y\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        x = x - 2 * lr * func.dx(x, y) + lr * func.dx(x_prev, y_prev)\n",
    "        \n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        y = y + 2 * lr * func.dy(x, y) - lr * func.dy(x_prev, y_prev)\n",
    "        x_prev, y_prev = x, y\n",
    "    return np.array(xy_values), np.array(loss_values)\n",
    "\n",
    "\n",
    "def cgd(func, lr, init_point, num_steps=100, **kwargs):\n",
    "    x, y = init_point\n",
    "    xy_values, loss_values = [], []\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        dx, dy = func.dx(x, y), func.dy(x, y)\n",
    "        dxy, dyx = func.dxy(x, y), func.dyx(x, y)\n",
    "        dxx, dyy = func.dxx(x, y), func.dyy(x, y)\n",
    "        \n",
    "        deltax, deltay = -linalg.solve(np.array([[1. / lr + dxx, dxy], [-dyx, 1. / lr - dyy]]), np.array([dx, -dy]))\n",
    "        x = x + deltax\n",
    "    \n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        dx, dy = func.dx(x, y), func.dy(x, y)\n",
    "        dxy, dyx = func.dxy(x, y), func.dyx(x, y)\n",
    "        dxx, dyy = func.dxx(x, y), func.dyy(x, y)\n",
    "        deltax, deltay = -linalg.solve(np.array([[1. / lr + dxx, dxy], [-dyx, 1. / lr - dyy]]), np.array([dx, -dy]))\n",
    "        y = y + deltay\n",
    "#         x = x - lr * (dx + lr * dxy * dy) / (1 + lr ** 2 * dxy * dyx)\n",
    "#         y = y - lr * (-dy + lr * dyx * dx) / (1 + lr ** 2 * dyx * dxy)\n",
    "    return np.array(xy_values), np.array(loss_values)\n",
    "\n",
    "\n",
    "def fr(func, lr, init_point, num_steps=100, **kwargs):\n",
    "    x, y = init_point\n",
    "    xy_values, loss_values = [], []\n",
    "    for _ in range(num_steps):\n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        x = x - lr * func.dx(x, y)\n",
    "        \n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        y = y + lr * func.dy(x, y) + lr * (1. / func.dyy(x, y)) * func.dyx(x, y) * func.dx(x, y)\n",
    "    return np.array(xy_values), np.array(loss_values)\n",
    "\n",
    "\n",
    "def co(func, lr, init_point, num_steps=100, **kwargs):\n",
    "    x, y = init_point\n",
    "    xy_values, loss_values = [], []\n",
    "    for _ in range(num_steps):\n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        x = x - lr * func.dx(x, y) - 2 * lr * kwargs['gamma'] * (func.dx(x, y) * func.dxx(x, y) + func.dy(x, y) * func.dyx(x, y))\n",
    "        \n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)        \n",
    "        y = y + lr * func.dy(x, y) - 2 * lr * kwargs['gamma'] * (func.dy(x, y) * func.dyy(x, y) + func.dx(x, y) * func.dxy(x, y))\n",
    "    return np.array(xy_values), np.array(loss_values)\n",
    "\n",
    "\n",
    "def fun(func, lr, init_point, num_steps=100, **kwargs):\n",
    "    x, y = init_point\n",
    "    xy_values, loss_values = [], []\n",
    "    for _ in range(num_steps):\n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "#         x = x - lr * func.dx(x, y) + lr * (1. / func.dxx(x, y)) * func.dxy(x, y) * func.dy(x, y)\n",
    "#         x = x - lr * func.dx(x, y)\n",
    "#         y = y + lr * func.dy(x, y) - lr * func.dyx(x, y) * func.dx(x, y)\n",
    "#         y = y + lr * func.dy(x, y)\n",
    "\n",
    "        x = x - lr * (func.dx(x, y) + kwargs['lambd'] * func.dxy(x, y) * func.dy(x, y))\n",
    "        loss = func.fn(x, y)\n",
    "        xy_values.append((x, y))\n",
    "        loss_values.append(loss)\n",
    "        y = y + lr * (func.dy(x, y) - kwargs['lambd'] * func.dyx(x, y) * func.dx(x, y))\n",
    "    return np.array(xy_values), np.array(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Function = namedtuple('Function', ['fn', 'dx', 'dy', 'dxx', 'dxy', 'dyx', 'dyy'])\n",
    "func = Function(\n",
    "    fn=lambda x, y: x * y,\n",
    "    dx=lambda x, y: y,\n",
    "    dy=lambda x, y: x,\n",
    "    dxx=lambda x, y: 0,\n",
    "    dxy=lambda x, y: 1,\n",
    "    dyx=lambda x, y: 1,\n",
    "    dyy=lambda x, y: 0,\n",
    ")\n",
    "plot_path(func, methods=['gda', 'sga', 'co', 'cgd'], lr=5e-2, lambd=1, gamma=0.5, init_point=(6, 6), num_steps=300, min_point=(0, 0), num_lines=60, xrange=(-15, 15), yrange=(-15, 15), elev=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Function = namedtuple('Function', ['fn', 'dx', 'dy', 'dxx', 'dxy', 'dyx', 'dyy'])\n",
    "func = Function(\n",
    "    fn=lambda x, y: 3 * x * x + y * y + 4 * x * y,\n",
    "    dx=lambda x, y: 6 * x + 4 * y,\n",
    "    dy=lambda x, y: 2 * y + 4 * x,\n",
    "    dxx=lambda x, y: 6,\n",
    "    dxy=lambda x, y: 4,\n",
    "    dyx=lambda x, y: 4,\n",
    "    dyy=lambda x, y: 2,\n",
    ")\n",
    "plot_path(func, methods=['gda', 'sga', 'eg', 'fr', 'co', 'ogda', 'cgd', 'fr'], lr=5e-2, lambd=1.0, gamma=0.5, init_point=(6, 6), num_steps=300, min_point=(0, 0), num_lines=60, xrange=(-15, 15), yrange=(-15, 15), elev=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Function = namedtuple('Function', ['fn', 'dx', 'dy', 'dxx', 'dxy', 'dyx', 'dyy'])\n",
    "func = Function(\n",
    "    fn=lambda x, y: np.log(1 + np.exp(x)) + 3 * x * y - np.log(1 + np.exp(y)),\n",
    "    dx=lambda x, y: 1. / (1. + np.exp(-x)) + 3 * y,\n",
    "    dy=lambda x, y: -1. / (1. + np.exp(-y)) + 3 * x,\n",
    "    dxx=lambda x, y: 1. / (1. + np.exp(-x)) - 1. / (1. + np.exp(-x)) ** 2,\n",
    "    dxy=lambda x, y: 3,\n",
    "    dyx=lambda x, y: 3,\n",
    "    dyy=lambda x, y: -1. / (1. + np.exp(-x)) + 1. / (1. + np.exp(-x)) ** 2,\n",
    ")\n",
    "fig = plot_path(func, methods=['fr', 'fun'], lr=5e-2, lambd=1.0, gamma=0.5, init_point=(6, 6), num_steps=300, min_point=(0, 0), num_lines=60, xrange=(-10, 10), yrange=(-10, 10), elev=30, figsize=(14, 3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Function = namedtuple('Function', ['fn', 'dx', 'dy', 'dxx', 'dxy', 'dyx', 'dyy'])\n",
    "func = Function(\n",
    "    fn=lambda x, y: np.exp(-0.01 * (x * x + y * y)) * (4 * x * x - (y - 3 * x + 0.05 * x ** 3) ** 2 - 0.1 * y ** 4),\n",
    "    dx=lambda x, y: np.exp(-0.01 * (x * x + y * y)) * (0.00005 * x ** 7 - 0.021 * x ** 5 + 0.002 * x ** 4 * y + 1.3 * x ** 3 - 0.42 * x ** 2 * y + x * (0.002 * y ** 4 + 0.02 * y ** 2 - 10) + 6 * y),\n",
    "    dy=lambda x, y: np.exp(-0.01 * (x * x + y * y)) * (0.00005 * x ** 6 * y - 0.006 * x ** 4 * y + x ** 3 * (0.002 * y ** 2 - 0.1) + 0.1 * x ** 2 * y + x * (6 - 0.12 * y ** 2) + 0.002 * y ** 5 - 0.38 * y ** 3 - 2 * y),\n",
    "    dxx=lambda x, y: np.exp(-0.01 * (x * x + y * y)) * (-0.04 * x * (8 * x - 2 * (0.15 * x ** 2 - 3) * (0.05 * x ** 3 - 3 * x + y)) + (-0.6 * x * (0.05 * x ** 3 - 3 * x + y) - 2 * (0.15 * x ** 2 - 3) ** 2 + 8) + 0.0004 * x ** 2 * (-(0.05 * x ** 3 - 3 * x + y) ** 2 + 4 * x ** 2 - 0.1 * y ** 4) - 0.02 * (-(0.05 * x ** 3 - 3 * x + y) ** 2 + 4 * x ** 2 - 0.1 * y ** 4)),\n",
    "    dxy=lambda x, y: np.exp(-0.01 * (x * x + y * y)) * (-2 * (0.15 * x ** 2 - 3) - 0.02 * y * (8 * x - 2 * (0.15 * x ** 2 - 3) * (0.05 * x ** 3 - 3 * x + y)) + 0.0004 * x * y * (-(0.05 * x ** 3 - 3 * x + y) ** 2 + 4 * x ** 2 - 0.1 * y ** 4) - 0.02 * x * (-2 * (0.05 * x ** 3 - 3 * x + y) - 0.4 * y ** 3)),\n",
    "    dyx=lambda x, y: np.exp(-0.01 * (x * x + y * y)) * (-2 * (0.15 * x ** 2 - 3) - 0.02 * y * (8 * x - 2 * (0.15 * x ** 2 - 3) * (0.05 * x ** 3 - 3 * x + y)) + 0.0004 * x * y * (-(0.05 * x ** 3 - 3 * x + y) ** 2 + 4 * x ** 2 - 0.1 * y ** 4) - 0.02 * x * (-2 * (0.05 * x ** 3 - 3 * x + y) - 0.4 * y ** 3)),\n",
    "    dyy=lambda x, y: np.exp(-0.01 * (x * x + y * y)) * ((-1.2 * y ** 2 - 2) + 0.0004 * y ** 2 * (-(0.05 * x ** 3 - 3 * x + y) ** 2 - 0.1 * y ** 4) - 0.02 * (-(0.05 * x ** 3 - 3 * x + y) ** 2 + 4 * x ** 2 - 0.1 * y ** 4) - 0.04 * y * (-2 * (0.05 * x ** 3 - 3 * x + y) - 0.4 * y ** 3)),\n",
    ")\n",
    "# plot_path(func, methods=['gda', 'sga', 'eg', 'fr', 'co', 'ogda', 'cgd'], lr=5e-2, lambd=1.0, gamma=0.03, init_point=(5, 5), num_steps=100, min_point=(0, 0), num_lines=60, xrange=(-7.5, 7.5), yrange=(-7.5, 7.5), elev=70)\n",
    "# plot_path(func, methods=['cgd', 'sga', 'co', 'gda'], lr=5e-2, lambd=1.0, gamma=0.03, init_point=(5, 5), num_steps=100, min_point=(0, 0), num_lines=60, xrange=(-7.5, 7.5), yrange=(-7.5, 7.5), elev=70)\n",
    "plot_path(func, methods=['gda', 'sga', 'fun'], lr=5e-2, lambd=5e-2, gamma=0.03, init_point=(5, 5), num_steps=100, min_point=(0, 0), num_lines=60, xrange=(-7.5, 7.5), yrange=(-7.5, 7.5), elev=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "\n",
    "def build_generator():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation=None),\n",
    "    ])\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(384, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation=None),\n",
    "    ])\n",
    "\n",
    "\n",
    "def data_generator(batch_size=256, sigma=0.02):\n",
    "    mus = np.mgrid[-1.5:2:1, -1.5:2:1].reshape(2, -1).T.astype(np.float32)\n",
    "    mus = np.tile(mus, (batch_size // 16 + 1, 1))[:batch_size].astype(np.float32)\n",
    "    return mus + sigma * tf.random.normal((batch_size, 2))\n",
    "\n",
    "\n",
    "def train(step_fn, num_iterations=10001):\n",
    "    generator = build_generator()\n",
    "    discriminator = build_discriminator()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=1e-4, beta_1=0.5, beta_2=0.9999)\n",
    "    \n",
    "    fixed_noise = tf.random.normal([2560, 64])\n",
    "    X, Y = np.mgrid[-3:3:0.03, -3:3:0.03]\n",
    "    fixed_grid = np.vstack([X.flatten(), Y.flatten()]).T.astype(np.float32)\n",
    "    fixed_samples = data_generator(2560).numpy()\n",
    "    \n",
    "    gen_losses, dis_losses = [], []\n",
    "    for it in range(num_iterations):\n",
    "        real_samples = data_generator()\n",
    "        gen_loss, dis_loss = step_fn(generator, discriminator, optimizer, real_samples)\n",
    "        gen_losses.append(gen_loss.numpy()[0])\n",
    "        dis_losses.append(dis_loss.numpy()[0])\n",
    "\n",
    "        if it % 1000 == 0:\n",
    "            x = generator(fixed_noise, training=False).numpy()\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 4))\n",
    "            sns.kdeplot(x[:, 0], x[:, 1], shade=True, ax=ax1, cmap='Reds')\n",
    "            ax1.set_title('Iteration {}'.format(it))\n",
    "\n",
    "            ax2.contour(X, Y, discriminator(fixed_grid).numpy().reshape(200, 200), levels=10)\n",
    "            ax2.scatter(fixed_samples[:, 0], fixed_samples[:, 1], s=5, alpha=0.2, c='C0')\n",
    "            ax2.scatter(x[:, 0], x[:, 1], s=5, alpha=0.2, c='C1')\n",
    "            ax2.set_title('Discriminator {:.4f}'.format(dis_losses[-1]))\n",
    "            \n",
    "            ax3.plot(range(len(gen_losses)), gen_losses, alpha=0.8, label='Generator')\n",
    "            ax3.plot(range(len(dis_losses)), dis_losses, alpha=0.8, label='Discriminator')\n",
    "            ax3.set_title('Generator {:.4f}'.format(gen_losses[-1]))\n",
    "            ax3.legend()\n",
    "\n",
    "            for ax in [ax1, ax2]:\n",
    "                ax.set_xlim([-3, 3])\n",
    "                ax.set_ylim([-3, 3])\n",
    "                ax.set_aspect('equal')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gradient_descent_ascent(generator, discriminator, optimizer, real_samples):\n",
    "    noise = tf.random.normal((256, 64))\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "        fake_samples = generator(noise)\n",
    "        real_outputs = discriminator(real_samples)\n",
    "        fake_outputs = discriminator(fake_samples)\n",
    "\n",
    "        gen_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(fake_outputs), fake_outputs, from_logits=True)\n",
    "        dis_loss_real = tf.keras.losses.binary_crossentropy(tf.ones_like(real_outputs), real_outputs, from_logits=True)\n",
    "        dis_loss_fake = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_outputs), fake_outputs, from_logits=True)\n",
    "        dis_loss = dis_loss_real + dis_loss_fake\n",
    "    \n",
    "    gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    dis_grads = dis_tape.gradient(dis_loss, discriminator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(dis_grads, discriminator.trainable_variables))\n",
    "    return gen_loss, dis_loss\n",
    "\n",
    "\n",
    "train(step_fn=gradient_descent_ascent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def concensus_optimization(generator, discriminator, optimizer, real_samples):\n",
    "    noise = tf.random.normal((256, 64))\n",
    "    with tf.GradientTape(persistent=True) as reg_tape:\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "            fake_samples = generator(noise)\n",
    "            real_outputs = discriminator(real_samples)\n",
    "            fake_outputs = discriminator(fake_samples)\n",
    "\n",
    "            gen_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(fake_outputs), fake_outputs, from_logits=True)\n",
    "            dis_loss_real = tf.keras.losses.binary_crossentropy(tf.ones_like(real_outputs), real_outputs, from_logits=True)\n",
    "            dis_loss_fake = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_outputs), fake_outputs, from_logits=True)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "        gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        dis_grads = dis_tape.gradient(dis_loss, discriminator.trainable_variables)\n",
    "        grad_norm = 0.5 * sum(tf.reduce_sum(tf.square(g)) for g in itertools.chain(gen_grads, dis_grads))\n",
    "        \n",
    "    gen_reg_grads = reg_tape.gradient(grad_norm, generator.trainable_variables)\n",
    "    dis_reg_grads = reg_tape.gradient(grad_norm, discriminator.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients((grad + 1. * reg, var) for grad, reg, var in zip(gen_grads, gen_reg_grads, generator.trainable_variables))\n",
    "    optimizer.apply_gradients((grad + 1. * reg, var) for grad, reg, var in zip(dis_grads, dis_reg_grads, discriminator.trainable_variables))\n",
    "    del reg_tape\n",
    "    return gen_loss, dis_loss\n",
    "\n",
    "\n",
    "train(step_fn=concensus_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def symplectic_gradient(generator, discriminator, optimizer, real_samples):\n",
    "    noise = tf.random.normal((256, 64))\n",
    "    with tf.GradientTape(persistent=True) as reg_tape:\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "            fake_samples = generator(noise)\n",
    "            real_outputs = discriminator(real_samples)\n",
    "            fake_outputs = discriminator(fake_samples)\n",
    "\n",
    "            gen_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(fake_outputs), fake_outputs, from_logits=True)\n",
    "            dis_loss_real = tf.keras.losses.binary_crossentropy(tf.ones_like(real_outputs), real_outputs, from_logits=True)\n",
    "            dis_loss_fake = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_outputs), fake_outputs, from_logits=True)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "        gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        dis_grads = dis_tape.gradient(dis_loss, discriminator.trainable_variables)\n",
    "        import pdb; pdb.set_trace()\n",
    "        variables = itertools.chain(generator.trainable_variables, discriminator.trainable_variables)\n",
    "        gradients = itertools.chain(gen_grads, dis_grads)\n",
    "    \n",
    "        temp = tf.ones_like(gradients)\n",
    "        grad_temp = reg_tape.gradient(gradients, variables, output_gradients=temp)\n",
    "        jacvec = reg_tape.gradient(grad_temp, temp, output_gradients=gradients)\n",
    "        \n",
    "        dydxs = reg_tape.gradients(gradients, variables, output_gradients=grads, stop_gradients=variables)\n",
    "        jactvec = [tf.zeros_like(var) if dydx is None else dydx for var, dydx in zip(variables, dydxs)]\n",
    "        \n",
    "        at_v = list_divide_scalar(list_subtract(ht_v, h_v), 2.)\n",
    "    \n",
    "    optimizer.apply_gradients((grad + 1. * 0.5 * (jv - jtv), var) for jv, jtv, grad, var in zip(jacvec, jacvectc, gradients, variables))\n",
    "    del reg_tape\n",
    "    return gen_loss, dis_loss\n",
    "\n",
    "\n",
    "train(step_fn=symplectic_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Roboto\";\n",
       "        font-weight: normal;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/roboto/roboto-regular.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Roboto\";\n",
       "        font-weight: normal;\n",
       "        font-style: italic;\n",
       "        src:url(\"../css/fonts/roboto/roboto-italic.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Roboto\";\n",
       "        font-weight: bold;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/roboto/roboto-bold.ttf\") format(\"truetype\");\n",
       "    }\n",
       "\n",
       "    @font-face {\n",
       "        font-family: \"Consolas\";\n",
       "        font-weight: normal;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/consolas/consolas-regular.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Consolas\";\n",
       "        font-weight: normal;\n",
       "        font-style: italic;\n",
       "        src:url(\"../css/fonts/consolas/consolas-italic.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Consolas\";\n",
       "        font-weight: bold;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/consolas/consolas-bold.ttf\") format(\"truetype\");\n",
       "    }\n",
       "\n",
       "    @font-face {\n",
       "        font-family: \"SF Mono\";\n",
       "        font-weight: normal;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/sfmono/sfmono-regular.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"SF Mono\";\n",
       "        font-weight: normal;\n",
       "        font-style: italic;\n",
       "        src:url(\"../css/fonts/sfmono/sfmono-italic.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"SF Mono\";\n",
       "        font-weight: bold;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/sfmono/sfmono-bold.ttf\") format(\"truetype\");\n",
       "    }\n",
       "\n",
       "    @font-face {\n",
       "        font-family: \"CMU Sans Serif\";\n",
       "        font-weight: normal;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/cmu/cmu-regular.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"CMU Sans Serif\";\n",
       "        font-weight: bold;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/cmu/cmu-bold.ttf\") format(\"truetype\");\n",
       "    }\n",
       "\n",
       "    /* Change text font */\n",
       "    div.text_cell_render {\n",
       "        font-family: CMU Sans Serif, monospace;\n",
       "        font-size: 16px;\n",
       "        -ms-text-size-adjust: 100%;\n",
       "        -webkit-text-size-adjust: 100%;\n",
       "        color: #24292e;\n",
       "        line-height: 1.3;\n",
       "    }\n",
       "\n",
       "    /* Change code font */\n",
       "    .CodeMirror pre {\n",
       "        font-family: SF Mono, monospace;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "        font-size: 12px;\n",
       "        line-height: 130%;\n",
       "    }\n",
       "\n",
       "    .CodeMirror-linenumber {\n",
       "        background: #fff;\n",
       "        font-family: SF Mono, monospace;\n",
       "        font-size: 11.5px;\n",
       "    }\n",
       "\n",
       "    /* Font changes for output needed for tqdm progress bar */\n",
       "    div.output_area pre {\n",
       "        font-family: SF Mono, monospace;\n",
       "        font-size: 11.5px;\n",
       "        padding-top: 0px;\n",
       "        padding-bottom: 0px;\n",
       "    }\n",
       "\n",
       "    div.output_subarea {\n",
       "        padding-top: 0px;\n",
       "        padding-bottom: 0px;\n",
       "        border: 0px;\n",
       "    }\n",
       "\n",
       "    /* Center plots */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* Font changes for widgets, useful for tqdm progress bar */\n",
       "    label.widget-label {\n",
       "        font-size: 12px;\n",
       "    }\n",
       "\n",
       "    div.p-Widget.jupyter-widgets.widget-inline-hbox.widget-html {\n",
       "        font-size: 12px;\n",
       "    }\n",
       "\n",
       "    div.jupyter-widgets-view {\n",
       "        font-family: SF Mono, monospace;\n",
       "    }\n",
       "\n",
       "    div.bk-root, div.bk-plot-wrapper, div.bk-canvas-overlays {\n",
       "        font-family: SF Mono, monospace;\n",
       "    }\n",
       "\n",
       "    div.bk-canvas-events {\n",
       "        display: none;\n",
       "    }\n",
       "\n",
       "    /* Syntax highlighting */\n",
       "    .cm-s-ipython span.cm-keyword {\n",
       "        color: #B43673;\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-number {\n",
       "        color: #3482f5;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-string {\n",
       "        color: #479035;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-def {\n",
       "        color: #000;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-builtin {\n",
       "        color: #790EAD;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-operator {\n",
       "        color: #B43673;\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-comment {\n",
       "        font-style: normal;\n",
       "        color: #927E41;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython div.CodeMirror-selected {\n",
       "        background: #D4E3FC !important;\n",
       "    }\n",
       "\n",
       "    /* Change debugging colors */\n",
       "    .ansi-green-fg {\n",
       "        color: #479035;\n",
       "    }\n",
       "\n",
       "    .ansi-cyan-fg {\n",
       "        color: #3482f5;\n",
       "    }\n",
       "\n",
       "    .ansi-cyan-fg {\n",
       "        color: #000;\n",
       "    }\n",
       "\n",
       "    .ansi-red-fg {\n",
       "        color: #c74230;\n",
       "    }\n",
       "\n",
       "    /* Change code font */\n",
       "    .rendered_html code {\n",
       "        font-family: SF Mono, monospace;\n",
       "    }\n",
       "\n",
       "    /* Disable prompt */\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "\n",
       "    /* Change font for headers */\n",
       "    div.text_cell_render h1,\n",
       "    div.text_cell_render h2,\n",
       "    div.text_cell_render h3,\n",
       "    div.text_cell_render h4,\n",
       "    div.text_cell_render h5,\n",
       "    div.text_cell_render h6 {\n",
       "        font-family: 'Roboto';\n",
       "        font-weight: 300;\n",
       "    }\n",
       "\n",
       "    div.text_cell_render h1 {\n",
       "        font-size: 20pt;\n",
       "    }\n",
       "\n",
       "    div.text_cell_render h2 {\n",
       "        font-size: 17pt;\n",
       "    }\n",
       "\n",
       "    div.text_cell_render h3 {\n",
       "        font-size: 14pt;\n",
       "    }\n",
       "\n",
       "    /* Changes for nbviewer */\n",
       "    div.container.container-main {\n",
       "        margin-right: auto;\n",
       "        margin-left: auto;\n",
       "        color:#333;\n",
       "        background:#fff;\n",
       "\n",
       "        width: 980px !important;\n",
       "        padding: 45px !important;\n",
       "        border: 1px solid #ddd;\n",
       "        border-radius: 3px;\n",
       "        word-wrap: break-word;\n",
       "    }\n",
       "    code, pre {\n",
       "        font-family: SF Mono, monospace !important;\n",
       "        padding-left: 8px !important;\n",
       "        padding-right: 8px !important;\n",
       "        font-size: 12px !important;\n",
       "        line-height: 130% !important;\n",
       "    }\n",
       "    body {\n",
       "        margin-top: 0px;\n",
       "        padding-top: 0px;\n",
       "    }\n",
       "    .navbar, footer, .breadcrumb {\n",
       "        display: none !important;\n",
       "    }\n",
       "    .highlight .k, .highlight .bp, .highlight .kn, .highlight .kc, .highlight .o, .highlight .ow {\n",
       "        color: #B43673 !important;\n",
       "        font-weight: normal !important;\n",
       "    }\n",
       "    .highlight .mi, .highlight .mf {\n",
       "        color: #3482f5 !important;\n",
       "        font-weight: normal !important;\n",
       "    }\n",
       "    .highlight .nn, .highlight .nc, .highlight .nf, .highlight .p, .highlight .n, .highlight .sa {\n",
       "        color: #000 !important;\n",
       "        font-weight: normal !important;\n",
       "    }\n",
       "    .highlight .s1, .highlight .s2, .highlight .si {\n",
       "        color: #479035 !important;\n",
       "        font-weight: normal !important;\n",
       "    }\n",
       "    .highlight .nb {\n",
       "        color: #790EAD !important;\n",
       "    }\n",
       "    .highlight .c1 {\n",
       "        color: #927E41 !important;\n",
       "        font-style: normal !important;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "        TeX: {\n",
       "            extensions: [\"AMSmath.js\"],\n",
       "            equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "        },\n",
       "        tex2jax: {\n",
       "            inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "            displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n",
       "            processEscapes: true,\n",
       "            processEnvironments: true\n",
       "        },\n",
       "        MathML: {\n",
       "            extensions: ['content-mathml.js']\n",
       "        },\n",
       "        \"HTML-CSS\": {\n",
       "            availableFonts: [\"TeX\"],\n",
       "            imageFont: null,\n",
       "            preferredFont: \"TeX\",\n",
       "            webFont: \"TeX\",\n",
       "            styles: {'.MathJax_Display': {\"margin\": \"10px\"}},\n",
       "            linebreaks: { automatic: true }\n",
       "        },\n",
       "    });\n",
       "    MathJax.Hub.Queue(\n",
       "        [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "        [\"PreProcess\", MathJax.Hub],\n",
       "        [\"Reprocess\", MathJax.Hub]\n",
       "    );\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(open('../css/custom.css', 'r').read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
