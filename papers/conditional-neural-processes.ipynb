{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### Paper Information\n", "\n", "* Title: [Conditional Neural Processes](https://arxiv.org/abs/1807.01613)\n", "* Authors: Marta Garnelo, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo J. Rezende, S. M. Ali Eslami (2018)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Main Ideas\n", "\n", "* Traditionally, deep neural networks learns a deterministic function and lacks the capacity to adapt to changes in the input distribution, which is important in certain applications such as meta-learning and continual learning.\n", "\n", "* A conditional neural process (CNP) models a stochastic process and more accurately a family of conditional distributions given a context set. Its architecture is simple; an average representation of the context points is concatenated to each target point to output a probabilistic prediction, whose negative log likelihood provides an objective function to minimize.\n", "\n", "* CNPs are scalable and capable of uncertainty estimation, although its variance estimates are not as accurate as Gaussian Processes."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div style=\"display:flex\">\n", "     <div style=\"flex:1;padding-right:5px;\">\n", "          <img src=\"images/conditional-neural-processes/figure-1.png\" alt=\"Drawing\" style=\"width: 80%;\"/>\n", "     </div>\n", "     <div style=\"flex:1;padding-left:5px;\">\n", "          <img src=\"images/conditional-neural-processes/figure-4.png\" alt=\"Drawing\" style=\"width: 90%;\"/>\n", "     </div>\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Methodology\n", "\n", "* Inspired by Gaussian Processes, CNPs define a conditional distribution $p(Y \\rvert X, C)$ for the target points $T = (X, Y) = \\{x_i, y_i\\}_{i \\in \\mathcal{I}(T)}$ given a set of context points $C = (X_C, Y_C) = \\{x_i, y_i\\}_{i \\in \\mathcal{I}(C)}$. The dependency on $C$ is specified via a deterministic representation $r_C = r(X_C, Y_C)$, which is practically formed by averaging hidden representations of the context points in $C$:\n", "\n", "    $$p(Y \\rvert X, C) = \\prod_{(x, y) \\in T} p(y; g_\\phi(x, r_C)), \\quad r_C = r_1 \\oplus r_2 \\cdots \\oplus r_{|\\mathcal{I}(C)|}, \\quad r_i = h_\\theta(x_i, y_i),$$\n", "\n", "    where $h_\\theta: X \\times Y \\rightarrow \\mathbb{R}^d$ and $g_\\phi: X \\times R^d \\rightarrow \\mathbb{R}^e$ are neural networks and $\\oplus$ denotes a commutative aggregation operation such as elementwise summation or multiplication For regression tasks, for example, $g_\\phi(x, r_C) = (\\mu_x, \\sigma_x^2)$ provides the mean and variance of a Gaussian distribution.\n", "\n", "* Despite its simplicity, the aggregation operator $\\oplus$ ensures $p(Y \\rvert X, C)$ is invariant under permutations of $C$ while its factorization form guarantees invariance under permutations of $T$. These two properties are built into the model to resemble those of a stochastic process. Still, a CNP is not necessarily a stochastic process since it generally fails to satisfy the consistency condition under marginalization.\n", "\n", "* Training a CNP amounts to minimizing the negative conditional log probability of the target points $\\mathcal{L}(\\theta, \\phi) = -\\log p(Y \\rvert X, C)$. Each training iteration is given a small number of context points similar to what happens at test time.\n", "\n", "* Compared to Gaussian Processes, CNPs are scalable, which has a running time complexity of $\\mathcal{O}(n + m)$ where $n$ and $m$ are the sizes of the context set and the target set.\n", "\n", "### Experiments\n", "\n", "* The first experiment involves function regression, where the target function is drawn from a fixed Gaussian process or a set of Gaussian processes of different kernel parameters. The second experiment involves image completion and binary pixel classification on MNIST and CIFAR10. Generally, the variance estimates around context points are small, capturing some notion of uncertainty. More context points result in better prediction and changes in variance estimates, though perfect reconstruction is not attainable possible due to the bottleneck at the representation level.\n", "\n", "* The third experiment involves one-shot classification on Omniglot, where each class has only 20 examples. CNPs do not outperform matching networks, although their results are somewhat comparable.\n", "\n", "### Related Works\n", "\n", "* CNPs are closely related to Generative Query Networks ([Eslami et al., 2018](https://deepmind.com/blog/article/neural-scene-representation-and-rendering)), a meta-learning approach that models conditional distributions to predict new viewpoints in 3D scenes. Deep generative models for one-shot generalization ([J. Rezende et al., 2016](https://arxiv.org/abs/1603.05106)) also conditions on context during inference.\n", "\n", "* Comparing the context points and the target points using some attention-based distance metric in feature space has also been done in Matching Networks ([Vinyals et al., 2016](https://arxiv.org/abs/1606.04080)) and Generative Matching Networks ([Bartunov and Vetro, 2016](https://arxiv.org/abs/1612.02192)). In contrast, CNPs do not explicitly define a distance kernel.\n", "\n", "### Critical Thoughts\n", "\n", "* Although the authors mention including latent variables in the experiment section, they are not an essential piece of the model. Latent variables allow for sampling multiple target distributions given the same context set as demonstrated in follow-up works, including Neural Processes ([Garnelo et al., 2018](https://arxiv.org/abs/1807.01622)) and Attentive Neural Processes ([Kim et al., 2019](https://arxiv.org/abs/1901.05761)).\n", "\n", "* CNPs seem to overestimate uncertainty around context points (i.e. underfitting) and underestimate uncertainty everywhere else (i.e. overconfidence). Also, the variance estimates are not very smooth, which is probably related to the choice of ReLU activations (see [this blog post](https://kasparmartens.rbind.io/post/np/)). It's also not clear how these variance estimates compare to those generate by traditional probabilistic models such as Bayesian networks or variational dropout.\n", "\n", "* Modeling a stochastic process with deep neural networks is an interesting idea. The authors consider learning high-level abstractions as a grand challenge in machine learning. Indeed, this paper sparks certain interests in functional approaches using deep nets (see Functional Neural Process ([Louizos et al. (2019)](https://arxiv.org/abs/1906.08324)), Sequential Neural Process ([Singh et al. (2019)](https://arxiv.org/abs/1906.10264)), for example).\n", "\n", "### Implementation\n", "\n", "The following implementation is based on the official implementation [deepmind/neural-processes](https://github.com/deepmind/neural-processes)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.distributions as D\n", "import torch.nn.functional as F\n", "\n", "import torchvision\n", "import torchvision.datasets as datasets\n", "import torchvision.transforms as transforms\n", "\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import random\n", "\n", "from tqdm.notebook import tqdm\n", "\n", "\n", "def gaussian_kernel(X1, X2, l1=0.4, sigma=1.0):\n", "    return sigma ** 2 * ((-0.5 / l1 ** 2) * (torch.cdist(X1, X2, p=2) ** 2)).exp()\n", "\n", "\n", "def sample_gaussian_process(inputs, l1=0.4, sigma=1.0, eps=2e-3):\n", "    # Compute covariance matrix of size B x M x M induced by a Gaussian kernel\n", "    batch_size, num_points, x_dim = inputs.size()\n", "    covariance = gaussian_kernel(inputs, inputs, l1=l1, sigma=sigma)\n", "\n", "    # Add diagonal noise to avoid singular covariance matrix\n", "    covariance += (eps ** 2) * torch.eye(num_points)\n", "    cholesky = torch.cholesky(covariance.double()).float()\n", "    outputs = torch.matmul(cholesky, torch.randn(batch_size, num_points, 1))\n", "    return outputs\n", "\n", "\n", "def generate_train_data(batch_size=16, max_num_context=50):\n", "    num_context = torch.randint(low=3, high=max_num_context, size=())\n", "    num_target = torch.randint(low=2, high=max_num_context, size=())\n", "    num_points = num_target + num_context\n", "\n", "    x_values = 4 * torch.rand(batch_size, num_points, 1) - 2\n", "    y_values = sample_gaussian_process(x_values)\n", "    x_target, y_target = x_values, y_values\n", "    x_context, y_context = x_values[:, :num_context], y_values[:, :num_context]\n", "    return x_context, y_context, x_target, y_target\n", "\n", "\n", "def generate_test_data(batch_size=1, max_num_context=50, num_target=400):\n", "    num_context = torch.randint(low=3, high=max_num_context, size=())\n", "    x_values = torch.linspace(-2, 2, 400).view(1, 400, 1).repeat((batch_size, 1, 1))\n", "    y_values = sample_gaussian_process(x_values)\n", "    x_target, y_target = x_values, y_values\n", "    idx = torch.randperm(num_target)[:num_context]\n", "    x_context, y_context = x_values[:, idx, :], y_values[:, idx, :]\n", "    return x_context, y_context, x_target, y_target\n", "\n", "\n", "def posterior_predictive(x_target, x_context, y_context, l1=0.4, sigma=1.0, eps=2e-3):\n", "    K = gaussian_kernel(x_context, x_context, l1=l1, sigma=sigma) + (eps ** 2) * torch.eye(len(x_context))\n", "    K_s = gaussian_kernel(x_context, x_target, l1=l1, sigma=sigma)\n", "    K_ss = gaussian_kernel(x_target, x_target, l1=l1, sigma=sigma)\n", "    K_inv = torch.inverse(K)\n", "\n", "    mu = K_s.transpose(1, 2).bmm(K_inv).bmm(y_context)\n", "    cov = K_ss - K_s.transpose(1, 2).bmm(K_inv).bmm(K_s)\n", "    std = (cov.diagonal(dim1=1, dim2=2) + 1e-6).sqrt()\n", "    return mu, std"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class LinearNormal(nn.Module):\n", "    def __init__(self, in_features, out_features):\n", "        super().__init__()\n", "        self.linear = nn.Linear(in_features, 2 * out_features)\n", "\n", "    def forward(self, inputs):\n", "        mean, std = self.linear(inputs).chunk(2, dim=-1)\n", "        std = 0.1 + 0.9 * F.softplus(std)\n", "        return D.Normal(loc=mean, scale=std)\n", "\n", "\n", "class ConditionalNeuralProcess(nn.Module):\n", "    def __init__(self, hidden_size=128, x_dim=1, y_dim=1):\n", "        super().__init__()\n", "        self.encoder = nn.Sequential(nn.Linear(x_dim + y_dim, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size),)\n", "        self.decoder = nn.Sequential(nn.Linear(hidden_size + x_dim, hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size), nn.ReLU(), LinearNormal(hidden_size, y_dim),)\n", "\n", "    def forward(self, x_context, y_context, x_target):\n", "        # x_context: [batch_size, num_context, x_dim]\n", "        # y_context: [batch_size, num_context, y_dim]\n", "        # x_target: [batch_size, num_points, x_dim]\n", "        # y_target: [batch_size, num_points, y_dim]\n", "\n", "        # Infer a global latent variable based on context input-output pairs\n", "        latent = self.encoder(torch.cat([x_context, y_context], dim=-1)).mean(dim=1)\n", "\n", "        # Concatenate the global latent variable to each target input\n", "        latent = latent[:, None, :].repeat(1, x_target.size(1), 1)\n", "        inputs = torch.cat([latent, x_target], dim=-1)\n", "\n", "        # Output a factorized distribution for target outputs\n", "        return self.decoder(inputs)\n", "\n", "\n", "model = ConditionalNeuralProcess().cuda()\n", "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n", "progress_bar = tqdm(range(int(1e5 + 1)), desc=\"Iteration\", leave=False)\n", "\n", "for it in progress_bar:\n", "    x_context, y_context, x_target, y_target = generate_train_data()\n", "    y_pred = model(x_context.cuda(), y_context.cuda(), x_target.cuda())\n", "    loss = -y_pred.log_prob(y_target.cuda()).mean()\n", "    model.zero_grad()\n", "    loss.backward()\n", "    optimizer.step()\n", "    progress_bar.set_postfix({\"loss\": \"{:.3f}\".format(loss.item())})\n", "\n", "    if it % 20000 == 0:\n", "        x_context, y_context, x_target, y_target = generate_test_data(max_num_context=10)\n", "        y_pred = model(x_context.cuda(), y_context.cuda(), x_target.cuda())\n", "        mu, sigma = y_pred.loc.detach().cpu().numpy(), y_pred.scale.detach().cpu().numpy()\n", "\n", "        fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n", "        ax.plot(x_target[0], mu[0], c=\"C0\", label=\"CNP\")\n", "        ax.scatter(x_context[0], y_context[0], c=\"r\", s=15, label=\"Context\")\n", "        ax.plot(x_target[0], y_target[0], c=\"C2\", label=\"Truth\")\n", "        ax.set_title(\"Iteration {}: loss {:.3f}\".format(it, loss.item()))\n", "        ax.fill_between(x_target[0, :, 0], mu[0, :, 0] - sigma[0, :, 0], mu[0, :, 0] + sigma[0, :, 0], alpha=0.2, facecolor='C0', interpolate=True)\n", "\n", "        mu, std = posterior_predictive(x_target, x_context, y_context)\n", "        ax.plot(x_target[0], mu[0], c='C1', label=\"GP\")\n", "        ax.fill_between(x_target[0].squeeze(), mu[0].squeeze() - std[0], mu[0].squeeze() + std[0], facecolor='C1', alpha=0.2)\n", "        ax.legend()\n", "\n", "        fig.savefig('images/conditional-neural-processes/iteration-{}.png'.format(it), dpi=300, bbox_inches='tight')\n", "        plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"images/conditional-neural-processes/cnp-toy.gif\" alt=\"Drawing\" style=\"width: 60%;\"/>"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "source": ["from IPython.core.display import HTML\n", "HTML(open('../css/custom.css', 'r').read())"], "outputs": [{"data": {"text/html": ["<style>\n", "    @font-face {\n", "        font-family: \"Roboto\";\n", "        font-weight: normal;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/roboto/roboto-regular.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"Roboto\";\n", "        font-weight: normal;\n", "        font-style: italic;\n", "        src:url(\"../css/fonts/roboto/roboto-italic.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"Roboto\";\n", "        font-weight: bold;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/roboto/roboto-bold.ttf\") format(\"truetype\");\n", "    }\n", "\n", "    @font-face {\n", "        font-family: \"Consolas\";\n", "        font-weight: normal;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/consolas/consolas-regular.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"Consolas\";\n", "        font-weight: normal;\n", "        font-style: italic;\n", "        src:url(\"../css/fonts/consolas/consolas-italic.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"Consolas\";\n", "        font-weight: bold;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/consolas/consolas-bold.ttf\") format(\"truetype\");\n", "    }\n", "\n", "    @font-face {\n", "        font-family: \"SF Mono\";\n", "        font-weight: normal;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/sfmono/sfmono-regular.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"SF Mono\";\n", "        font-weight: normal;\n", "        font-style: italic;\n", "        src:url(\"../css/fonts/sfmono/sfmono-italic.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"SF Mono\";\n", "        font-weight: bold;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/sfmono/sfmono-bold.ttf\") format(\"truetype\");\n", "    }\n", "\n", "    @font-face {\n", "        font-family: \"CMU Sans Serif\";\n", "        font-weight: normal;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/cmu/cmu-regular.ttf\") format(\"truetype\");\n", "    }\n", "    @font-face {\n", "        font-family: \"CMU Sans Serif\";\n", "        font-weight: bold;\n", "        font-style: normal;\n", "        src:url(\"../css/fonts/cmu/cmu-bold.ttf\") format(\"truetype\");\n", "    }\n", "\n", "    /* Change text font */\n", "    div.text_cell_render {\n", "        font-family: CMU Sans Serif, monospace;\n", "        font-size: 16px;\n", "        -ms-text-size-adjust: 100%;\n", "        -webkit-text-size-adjust: 100%;\n", "        color: #24292e;\n", "        line-height: 1.3;\n", "    }\n", "\n", "    /* Change code font */\n", "    .CodeMirror pre {\n", "        font-family: SF Mono, monospace;\n", "        padding-left: 8px;\n", "        padding-right: 8px;\n", "        font-size: 12px;\n", "        line-height: 130%;\n", "    }\n", "\n", "    .CodeMirror-linenumber {\n", "        background: #fff;\n", "        font-family: SF Mono, monospace;\n", "        font-size: 11.5px;\n", "    }\n", "\n", "    /* Font changes for output needed for tqdm progress bar */\n", "    div.output_area pre {\n", "        font-family: SF Mono, monospace;\n", "        font-size: 11.5px;\n", "        padding-top: 5px;\n", "        padding-bottom: 5px;\n", "    }\n", "\n", "    div.output_subarea {\n", "        padding-top: 0px;\n", "        padding-bottom: 0px;\n", "        border: 0px;\n", "    }\n", "\n", "    /* Center plots */\n", "    .output_png {\n", "        display: table-cell;\n", "        text-align: center;\n", "        vertical-align: middle;\n", "    }\n", "\n", "    /* Font changes for widgets, useful for tqdm progress bar */\n", "    label.widget-label {\n", "        font-size: 12px;\n", "    }\n", "\n", "    div.p-Widget.jupyter-widgets.widget-inline-hbox.widget-html {\n", "        font-size: 12px;\n", "    }\n", "\n", "    div.jupyter-widgets-view {\n", "        font-family: SF Mono, monospace;\n", "    }\n", "\n", "    div.bk-root, div.bk-plot-wrapper, div.bk-canvas-overlays {\n", "        font-family: SF Mono, monospace;\n", "    }\n", "\n", "    div.bk-canvas-events {\n", "        display: none;\n", "    }\n", "\n", "    /* Syntax highlighting */\n", "    .cm-s-ipython span.cm-keyword {\n", "        color: #B43673;\n", "        font-weight: normal;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-number {\n", "        color: #3482f5;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-string {\n", "        color: #479035;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-def {\n", "        color: #000;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-builtin {\n", "        color: #790EAD;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-operator {\n", "        color: #B43673;\n", "        font-weight: normal;\n", "    }\n", "\n", "    .cm-s-ipython span.cm-comment {\n", "        font-style: normal;\n", "        color: #927E41;\n", "    }\n", "\n", "    .cm-s-ipython div.CodeMirror-selected {\n", "        background: #D4E3FC !important;\n", "    }\n", "\n", "    /* Change debugging colors */\n", "    .ansi-green-fg {\n", "        color: #479035;\n", "    }\n", "\n", "    .ansi-cyan-fg {\n", "        color: #3482f5;\n", "    }\n", "\n", "    .ansi-cyan-fg {\n", "        color: #000;\n", "    }\n", "\n", "    .ansi-red-fg {\n", "        color: #c74230;\n", "    }\n", "\n", "    /* Change code font */\n", "    .rendered_html code {\n", "        font-family: SF Mono, monospace;\n", "    }\n", "\n", "    /* Disable prompt */\n", "    .prompt{\n", "        display: None;\n", "    }\n", "\n", "    /* Change font for headers */\n", "    div.text_cell_render h1,\n", "    div.text_cell_render h2,\n", "    div.text_cell_render h3,\n", "    div.text_cell_render h4,\n", "    div.text_cell_render h5,\n", "    div.text_cell_render h6 {\n", "        font-weight: 300;\n", "    }\n", "\n", "    div.text_cell_render h1 {\n", "        font-size: 20pt;\n", "    }\n", "\n", "    div.text_cell_render h2 {\n", "        font-size: 17pt;\n", "    }\n", "\n", "    div.text_cell_render h3 {\n", "        font-size: 16pt;\n", "    }\n", "\n", "    /* Changes for nbviewer */\n", "    div.container.container-main {\n", "        margin-right: auto;\n", "        margin-left: auto;\n", "        color:#333;\n", "        background:#fff;\n", "\n", "        width: 980px !important;\n", "        padding: 45px !important;\n", "        border: 1px solid #ddd;\n", "        border-radius: 3px;\n", "        word-wrap: break-word;\n", "    }\n", "    code, pre {\n", "        font-family: SF Mono, monospace !important;\n", "        padding-left: 8px !important;\n", "        padding-right: 8px !important;\n", "        font-size: 12px !important;\n", "        line-height: 140% !important;\n", "        overflow-x: scroll !important;\n", "    }\n", "    body {\n", "        margin-top: 0px;\n", "        padding-top: 0px;\n", "    }\n", "    .navbar, footer, .breadcrumb {\n", "        display: none !important;\n", "    }\n", "    .highlight .k, .highlight .bp, .highlight .kn, .highlight .kc, .highlight .o, .highlight .ow {\n", "        color: #B43673 !important;\n", "        font-weight: normal !important;\n", "    }\n", "    .highlight .mi, .highlight .mf {\n", "        color: #3482f5 !important;\n", "        font-weight: normal !important;\n", "    }\n", "    .highlight .nn, .highlight .nc, .highlight .nf, .highlight .p, .highlight .n, .highlight .sa {\n", "        color: #000 !important;\n", "        font-weight: normal !important;\n", "    }\n", "    .highlight .s1, .highlight .s2, .highlight .si, .highlight .sd {\n", "        color: #479035 !important;\n", "        font-weight: normal !important;\n", "        font-style: normal !important;\n", "    }\n", "    .highlight .nb {\n", "        color: #790EAD !important;\n", "    }\n", "    .highlight .c1 {\n", "        color: #927E41 !important;\n", "        font-style: normal !important;\n", "    }\n", "</style>\n", "\n", "\n", "<script>\n", "    MathJax.Hub.Config({\n", "        TeX: {\n", "            extensions: [\"AMSmath.js\"],\n", "            equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n", "        },\n", "        tex2jax: {\n", "            inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n", "            displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n", "            processEscapes: true,\n", "            processEnvironments: true\n", "        },\n", "        MathML: {\n", "            extensions: ['content-mathml.js']\n", "        },\n", "        \"HTML-CSS\": {\n", "            availableFonts: [\"TeX\"],\n", "            imageFont: null,\n", "            preferredFont: \"TeX\",\n", "            webFont: \"TeX\",\n", "            styles: {'.MathJax_Display': {\"margin\": \"10px\"}},\n", "            linebreaks: { automatic: true }\n", "        },\n", "    });\n", "    MathJax.Hub.Queue(\n", "        [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n", "        [\"PreProcess\", MathJax.Hub],\n", "        [\"Reprocess\", MathJax.Hub]\n", "    );\n", "</script>\n"], "text/plain": ["<IPython.core.display.HTML object>"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}}, "nbformat": 4, "nbformat_minor": 2}