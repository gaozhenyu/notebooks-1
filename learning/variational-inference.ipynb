{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Variable Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to model the probability distribution $p_\\theta (x)$ parametrized by $\\theta$, but it happens that $x$ depends on some latent variable $z$. For instance, if $x$ is an image of a handwritten digit, then $z$ can describe the digit number or the thickness of the writing. In that case,\n",
    "\\\\[ p_\\theta(x) = \\int p_\\theta (x | z) \\, p(z) \\, dz = \\mathbb{E}_{p(z)} \\left[ p_\\theta (x | z) \\right].\\\\]\n",
    "To find the maximum likelihood estimate $\\hat{\\theta} = \\text{argmax}_{\\theta \\in \\Theta} \\; p_\\theta(x)$, we need to evaluate the integral over $z$. However, $z$ can be high dimensional, and we would need many Monte Carlo samples of $z^{(m)} \\sim p(z)$ to get a decent approximation of the integral. Another better idea is to rely on [importance sampling](https://en.wikipedia.org/wiki/Importance_sampling), where we sample from a proposal distribution $q_\\phi (z | x)$ parametrized by $\\phi$ instead of directly from $p(z)$:\n",
    "\\\\[ p_\\theta(x) = \\int \\frac{p_\\theta (x | z) \\, p(z)}{q_\\phi (z | x)} q_\\phi(z | x) \\, dz = \\mathbb{E}_{q_\\phi(z | x)} \\left[ \\frac{p_\\theta (x | z) \\, p(z)}{q_\\phi (z | x)} \\right] \\approx \\frac{1}{M} \\sum_{m=1}^M  \\frac{p_\\theta (x | z^{(m)}) \\, p(z^{(m)}}{q_\\phi (z^{(m)} | x)} .\\\\]\n",
    "An important property of importance sampling is that it provides a lower bound on the log likelihood:\n",
    "\\\\[ \\log p_\\theta(x) = \\log \\mathbb{E}_{q_\\phi(z | x)} \\left[ \\frac{p_\\theta (x | z) \\, p(z)}{q_\\phi (z | x)} \\right] \\geq \\mathbb{E}_{q_\\phi(z | x)} \\log \\left[ \\frac{p_\\theta (x | z) \\, p(z)}{q_\\phi (z | x)} \\right] \\approx -\\log(M) + \\sum_{m=1}^M \\left[ \\log p_\\theta (x | z^{(m)}) + \\log p(z^{(m)}) - \\log q_\\phi (z^{(m)} | x) \\right].\\\\]\n",
    "The inequality comes from the concavity of the logarithmic function and [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality). Another illuminating way to confirm the inequality is via computing the difference between the left-hand side and the right-hand side:\n",
    "\\\\[ \\log p_\\theta(x) - \\mathbb{E}_{q_\\phi(z | x)} \\log \\left[ \\frac{p_\\theta (x | z) \\, p(z)}{q_\\phi (z | x)} \\right] = \\mathbb{E}_{q_\\phi(z | x)} \\log \\left[ \\frac{p_\\theta(x) q_\\phi (z | x)  } {p_\\theta (x | z) \\, p(z)} \\right] = \\mathbb{E}_{q_\\phi(z | x)} \\left[ \\frac{q_\\phi (z | x)}{p_\\phi (z | x)} \\right] = \\text{KL}(q_\\phi (z | x) \\, \\| \\, p_\\phi (z | x)) \\geq 0. \\\\]\n",
    "Here, $\\text{KL}(q_\\phi (z | x) \\, \\| \\, p_\\phi (z | x))$ denotes the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence) between the proposal distribution $q_\\phi (z | x)$ and $p_\\phi (z | x)$, which is always bounded below by 0, according to [Gibb's inequality](https://en.wikipedia.org/wiki/Gibbs%27_inequality). In the language of Bayesian inference, $p_\\phi (z | x)$ is the posterior distribution if we assume a prior $p(z)$ on the latent space. Since the posterior is often intractable, we use $q_\\phi (z | x)$ to approximate it, hence the name approximate posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the approximation is quite accurate, i.e. $\\text{KL}(q_\\phi (z | x) \\, \\| \\, p_\\phi (z | x)) \\approx 0$, maximizing the log likelihood $\\log p_\\theta(x)$ is now equivalent to maximizing the quantity on the right-hand side of the previous inequality, often known as the evidence lower bound (ELBO). It can be decomposed into two interpretable parts:\n",
    "\\\\[\\mathbb{E}_{q_\\phi(z | x)} \\log \\left[ \\frac{p_\\theta (x | z) \\, p(z)}{q_\\phi (z | x)} \\right] = \\mathbb{E}_{q_\\phi(z | x)} \\log p_\\theta (x | z) -  \\mathbb{E}_{q_\\phi(z | x)} \\log \\left[ \\frac{q_\\phi (z | x)}{p(z)} \\right] = \\mathbb{E}_{q_\\phi(z | x)} \\log p_\\theta (x | z) - \\text{KL} ( q_\\phi(z | x) \\, \\| \\, p(z)).\\\\]\n",
    "The first term $\\mathbb{E}_{q_\\phi(z | x)} \\log p_\\theta (x | z)$ describes the reconstruction error resulted from mapping the input $x$ to its latent code $z$ and back to the input space. The second term $\\text{KL} ( q_\\phi(z | x) \\, \\| \\, p(z))$ tells us how much the approximate posterior $q_\\phi(z | x)$ differs from the prior $p(z)$.\n",
    "\n",
    "We can get rid of the expectation in ELBO simply by using a single example from the proposal distribution to estimate it. Given a training dataset $\\mathcal{D}$, we find estimates for $\\theta \\in \\Theta$ and $\\phi \\in \\Phi$ using maximum likelihood, i.e. $\\theta^*, \\, \\phi^* = \\text{argmax}_{\\theta \\in \\Theta, \\, \\phi \\in \\Phi} \\mathcal{L}(x; \\theta, \\phi)$ where\n",
    "\\\\[ \\mathcal{L}(x; \\theta, \\phi) = \\frac{1}{|\\mathcal{D}|} \\sum_{i = 1}^{|\\mathcal{D}|} \\log \\left[ \\frac{p_\\theta (x | z) \\, p(z)}{q_\\phi (z | x)} \\right], \\quad z \\sim q_\\phi (z | x).\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably want to use gradient descent or any other optimization algorithms in that family to learn $\\theta$ and $\\phi$. To do so, we need to be able to compute gradients of $\\mathcal{L}(x; \\theta, \\phi)$ with respect to $\\theta$ and $\\phi$. Computing $\\nabla_\\theta \\mathcal{L}(x; \\theta, \\phi)$ is straightforward as long as $p_\\theta(x | z)$ is differentiable. Computing $\\nabla_\\phi \\mathcal{L}(x; \\theta, \\phi)$, however, is not straightforward, because the sampling distribution depends on $\\phi$. More generally, how do we actually compute $\\nabla_\\phi \\mathbb{E}_{q_\\phi(z)} \\left[ f (z) \\right]$?\n",
    "\n",
    "The first trick is reparametrization, probably best known in [Kingma and Welling](https://arxiv.org/abs/1312.6114). The idea is to parametrize $\\mathbb{E}_{q_\\phi(z)} \\left[ f(z) \\right]$ as $\\mathbb{E}_{q'(\\epsilon)} \\left[ f(g_\\phi(\\epsilon)) \\right]$. For example, if $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ (in this case $\\phi = \\{\\mu, \\sigma\\}$), then we can let $z = \\mu + \\epsilon \\sigma$ and sample $\\epsilon$ from $\\mathcal{N}(0, 1)$. Essentially, we let the function $f(z)$ absorb the parameters $\\phi$ while freeing the sampling distribution from them. Obviously, this trick only works if there is such an invertible mapping between $z$ and $\\epsilon$. In case the sampling distribution is Bernoulli or the function $f$ is non-differentiable, for example, the trick simply doesn't work.\n",
    "\n",
    "The second trick is with score function. [A score function](https://en.wikipedia.org/wiki/Score_(statistics) is a special likelihood function, often defined as $\\nabla_\\phi q_\\phi(x)$. Rewriting the gradient of ELBO with respect to $\\phi$ gives us \n",
    "\\\\[ \\nabla_\\phi \\mathbb{E}_{q_\\phi(z)} \\left[ f(z) \\right] \\stackrel{(1)}{=} \\int f(z) \\, \\nabla_\\phi q_\\phi(z) dz = \\int f(z) \\nabla_\\phi q_\\phi(z) \\left(\\frac{q_\\phi(z)}{q_\\phi(z)} \\right)  dz \\stackrel{(2)}{=} \\int f(z) \\, \\nabla_\\phi \\log q_\\phi(z) \\, q_\\phi(z)dz =  \\mathbb{E}_{q_\\phi(z)} \\left[ f (z) \\nabla_\\phi \\log q_\\phi(z) \\right] \\stackrel{(3)}{\\approx} \\, \\frac{1}{M} \\sum_{m = 1}^M f(z^{(m)}) \\nabla_\\phi \\log q_\\phi z^{(m)}. \\\\]\n",
    "\n",
    "Here, we (1) apply [Leibniz integral rule](https://en.wikipedia.org/wiki/Leibniz_integral_rule) to exchange the derivative and the integral, and (2) use a log-derivative trick to introduce another $q_\\phi(z)$ term, and (3) perform Monte Carlo approximation. Note that although it's tempted to just take the average of the gradients weighted by $f(z)$, that's not a valid approximation for the integral:\n",
    "\\\\[\\nabla_\\phi \\mathbb{E}_{q_\\phi(z)} \\left[ f (z) \\right] =  \\nabla_\\phi \\int f(z) q_\\phi(z) dz = \\int f(z) \\left( \\nabla_\\phi q_\\phi(z) \\right) dz \\, \\not\\approx \\, \\frac{1}{M} \\sum_{m = 1}^M f(z^{(m)}) \\nabla_\\phi q_\\phi(z^{(m)}).\\\\]\n",
    "\n",
    "The downside of the second trick is that the Monte Carlo approximation often has high variance. We can't simply get more samples to reduce the variance because it doesn't scale well. A better idea is to use [control variates](https://en.wikipedia.org/wiki/Control_variates). In this case, a good control variate is $C(x) \\nabla_\\phi \\log q_\\phi(z)$, for it can be shown that $\\mathbb{E}_{q_\\phi(z)} \\left[ C(x) \\nabla_\\phi \\log q_\\phi(z) \\right] = 0$ using the exact same trick as above. Putting everything together, we get a gradient estimator with lower variance that works even when no reparametrization is available:\n",
    "\n",
    "\\\\[\\nabla_\\phi \\mathbb{E}_{q_\\phi(z)} \\left[ f (z) \\right] = \\mathbb{E}_{q_\\phi(z)} \\left[ f (z) \\nabla_\\phi \\log q_\\phi(z) \\right] = \\mathbb{E}_{q_\\phi(z)} \\left[ (f(z) - C(x)) \\nabla_\\phi \\log q_\\phi(z) \\right] \\approx \\, \\frac{1}{M} \\sum_{m = 1}^M (f(z^{(m)}) - C(x)) \\nabla_\\phi \\log q_\\phi z^{(m)}.\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Roboto\";\n",
       "        font-weight: normal;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/roboto/roboto-regular.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Roboto\";\n",
       "        font-weight: normal;\n",
       "        font-style: italic;\n",
       "        src:url(\"../css/fonts/roboto/roboto-italic.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Roboto\";\n",
       "        font-weight: bold;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/roboto/roboto-bold.ttf\") format(\"truetype\");\n",
       "    }\n",
       "\n",
       "    @font-face {\n",
       "        font-family: \"Consolas\";\n",
       "        font-weight: normal;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/consolas/consolas-regular.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Consolas\";\n",
       "        font-weight: normal;\n",
       "        font-style: italic;\n",
       "        src:url(\"../css/fonts/consolas/consolas-italic.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Consolas\";\n",
       "        font-weight: bold;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/consolas/consolas-bold.ttf\") format(\"truetype\");\n",
       "    }\n",
       "\n",
       "    @font-face {\n",
       "        font-family: \"SF Mono\";\n",
       "        font-weight: normal;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/sfmono/sfmono-regular.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"SF Mono\";\n",
       "        font-weight: normal;\n",
       "        font-style: italic;\n",
       "        src:url(\"../css/fonts/sfmono/sfmono-italic.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"SF Mono\";\n",
       "        font-weight: bold;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/sfmono/sfmono-bold.ttf\") format(\"truetype\");\n",
       "    }\n",
       "\n",
       "    @font-face {\n",
       "        font-family: \"CMU Sans Serif\";\n",
       "        font-weight: normal;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/cmu/cmu-regular.ttf\") format(\"truetype\");\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"CMU Sans Serif\";\n",
       "        font-weight: bold;\n",
       "        font-style: normal;\n",
       "        src:url(\"../css/fonts/cmu/cmu-bold.ttf\") format(\"truetype\");\n",
       "    }\n",
       "\n",
       "    /* Change text font */\n",
       "    div.text_cell_render {\n",
       "        font-family: CMU Sans Serif, monospace;\n",
       "        font-size: 15px;\n",
       "        height: 100%;\n",
       "    }\n",
       "\n",
       "    /* Change code font */\n",
       "    .CodeMirror pre {\n",
       "        font-family: SF Mono, monospace;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "        font-size: 12px;\n",
       "        line-height: 130%;\n",
       "    }\n",
       "\n",
       "    .CodeMirror-linenumber {\n",
       "        background: #fff;\n",
       "        font-family: SF Mono, monospace;\n",
       "        font-size: 11.5px;\n",
       "    }\n",
       "\n",
       "    /* Font changes for output needed for tqdm progress bar */\n",
       "    div.output_area pre {\n",
       "        font-family: SF Mono, monospace;\n",
       "        font-size: 11.5px;\n",
       "        padding-top: 0px;\n",
       "        padding-bottom: 0px;\n",
       "    }\n",
       "\n",
       "    div.output_subarea {\n",
       "        padding-top: 0px;\n",
       "        padding-bottom: 0px;\n",
       "        border: 0px;\n",
       "    }\n",
       "\n",
       "    /* Center plots */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* Font changes for widgets, useful for tqdm progress bar */\n",
       "    label.widget-label {\n",
       "        font-size: 12px;\n",
       "    }\n",
       "\n",
       "    div.p-Widget.jupyter-widgets.widget-inline-hbox.widget-html {\n",
       "        font-size: 12px;\n",
       "    }\n",
       "\n",
       "    div.jupyter-widgets-view {\n",
       "        font-family: SF Mono, monospace;\n",
       "    }\n",
       "\n",
       "    div.bk-root, div.bk-plot-wrapper, div.bk-canvas-overlays {\n",
       "        font-family: SF Mono, monospace;\n",
       "    }\n",
       "\n",
       "    div.bk-canvas-events {\n",
       "        display: none;\n",
       "    }\n",
       "\n",
       "    /* Syntax highlighting */\n",
       "    .cm-s-ipython span.cm-keyword {\n",
       "        color: #B43673;\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-number {\n",
       "        color: #3482f5;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-string {\n",
       "        color: #479035;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-def {\n",
       "        color: #000;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-builtin {\n",
       "        color: #790EAD;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-operator {\n",
       "        color: #B43673;\n",
       "        font-weight: normal;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython span.cm-comment {\n",
       "        font-style: normal;\n",
       "        color: #927E41;\n",
       "    }\n",
       "\n",
       "    .cm-s-ipython div.CodeMirror-selected {\n",
       "        background: #D4E3FC !important;\n",
       "    }\n",
       "\n",
       "    /* Change debugging colors */\n",
       "    .ansi-green-fg {\n",
       "        color: #479035;\n",
       "    }\n",
       "\n",
       "    .ansi-cyan-fg {\n",
       "        color: #3482f5;\n",
       "    }\n",
       "\n",
       "    .ansi-cyan-fg {\n",
       "        color: #000;\n",
       "    }\n",
       "\n",
       "    .ansi-red-fg {\n",
       "        color: #c74230;\n",
       "    }\n",
       "\n",
       "    /* Change code font */\n",
       "    .rendered_html code {\n",
       "        font-family: SF Mono, monospace;\n",
       "    }\n",
       "\n",
       "    /* Disable prompt */\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "\n",
       "    /* Make container bigger */\n",
       "    .container {\n",
       "        width:95% !important;\n",
       "    }\n",
       "\n",
       "    /* Change font for headers */\n",
       "    div.text_cell_render h1,\n",
       "    div.text_cell_render h2,\n",
       "    div.text_cell_render h3,\n",
       "    div.text_cell_render h4,\n",
       "    div.text_cell_render h5,\n",
       "    div.text_cell_render h6 {\n",
       "        font-family: 'Roboto';\n",
       "        font-weight: 300;\n",
       "    }\n",
       "\n",
       "    div.text_cell_render h1 {\n",
       "        font-size: 20pt;\n",
       "    }\n",
       "\n",
       "    div.text_cell_render h2 {\n",
       "        font-size: 17pt;\n",
       "    }\n",
       "\n",
       "    div.text_cell_render h3 {\n",
       "        font-size: 14pt;\n",
       "    }\n",
       "\n",
       "    /* Changes for nbviewer */\n",
       "    code, pre {\n",
       "        font-family: SF Mono, monospace !important;\n",
       "        padding-left: 8px !important;\n",
       "        padding-right: 8px !important;\n",
       "        font-size: 12px !important;\n",
       "        line-height: 130% !important;\n",
       "    }\n",
       "    .highlight .k, .highlight .bp, .highlight .kn, .highlight .kc, .highlight .o, .highlight .ow {\n",
       "        color: #B43673 !important;\n",
       "        font-weight: normal !important;\n",
       "    }\n",
       "    .highlight .mi, .highlight .mf {\n",
       "        color: #3482f5 !important;\n",
       "        font-weight: normal !important;\n",
       "    }\n",
       "    .highlight .nn, .highlight .nc, .highlight .nf, .highlight .p, .highlight .n, .highlight .sa {\n",
       "        color: #000 !important;\n",
       "        font-weight: normal !important;\n",
       "    }\n",
       "    .highlight .s1, .highlight .s2, .highlight .si {\n",
       "        color: #479035 !important;\n",
       "        font-weight: normal !important;\n",
       "    }\n",
       "    .highlight .nb {\n",
       "        color: #790EAD !important;\n",
       "    }\n",
       "    .highlight .c1 {\n",
       "        color: #927E41 !important;\n",
       "        font-style: normal !important;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "        TeX: {\n",
       "            extensions: [\"AMSmath.js\"],\n",
       "            equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "        },\n",
       "        tex2jax: {\n",
       "            inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "            displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n",
       "            processEscapes: true,\n",
       "            processEnvironments: true\n",
       "        },\n",
       "        MathML: {\n",
       "            extensions: ['content-mathml.js']\n",
       "        },\n",
       "        \"HTML-CSS\": {\n",
       "            availableFonts: [\"TeX\"],\n",
       "            imageFont: null,\n",
       "            preferredFont: \"TeX\",\n",
       "            webFont: \"TeX\",\n",
       "            styles: {'.MathJax_Display': {\"margin\": \"10px\"}},\n",
       "            linebreaks: { automatic: true }\n",
       "        },\n",
       "    });\n",
       "    MathJax.Hub.Queue(\n",
       "        [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
       "        [\"PreProcess\", MathJax.Hub],\n",
       "        [\"Reprocess\", MathJax.Hub]\n",
       "    );\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(open('../css/custom.css', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
